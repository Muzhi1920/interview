# 逻辑回归

[逻辑回归面试整理](https://www.cnblogs.com/ModifyRong/p/7739955.html)

逻辑回归假设数据服从伯努利分布,通过极大化似然函数的方法，运用梯度下降来求解参数，来达到将数据二分类的目的。

**求解过程：按照极大似然估计求解或者交叉熵损失求解殊途同归**

- 1> 多个时间概率相乘得到似然表达式，值的大小意味着这组样本值都发生的可能性的大小；
- 2> 对似然表达式求导，必要时进行预处理，比如取对数（逻辑回归需要），令其导数为0，得到似然方程。
- 3> 求解似然方程，得到的参数解即为极大似然估计的解。
>>何为极大似然：“模型已定，参数未知”。通过若干次试验，观察其结果，反推得到某个参数值能够使样本出现的概率为最大，则称为极大似然估计。**利用已知的样本结果，反推最有可能（最大概率）导致这样结果的参数值。**

---
## 1、逻辑回归的假设
逻辑回归的第一个基本假设是假设数据服从伯努利分布。伯努利分布有一个简单的例子是抛硬币，抛中为正面的概率是$p$,抛中为负面的概率是$1−p$.在逻辑回归这个模型里面是假设$hθ(x)$为样本为正的概率，$1−h_θ(x)$为样本为负的概率。逻辑回归的最终形式：
$$h_\theta\left(x;\theta \right )=\frac{1}{1+e^{-\theta^{T} x}}$$


## 2、逻辑回归的损失函数
>>样本总体的后验概率
$$L_\theta\left(x\right )= \prod _{i=1}^{m}h_\theta(x_{i};\theta )^{y_{i}}*(1-h_\theta(x_{i};\theta))^{1-y_{i}}$$
>>损失函数为负对数损失时求和
![avatar](https://upload-images.jianshu.io/upload_images/4155986-905d8551ad03e16f.png?imageMogr2/auto-orient/strip|imageView2/2/w/1094/format/webp)

## 3、逻辑回归的求解方法
对应着三种优化方法：随机梯度下降，批梯度下降，small batch 梯度下降三种方式：
1. 批梯度下降会获得全局最优解，缺点是在更新每个参数的时候需要遍历所有的数据，计算量会很大，并且会有很多的冗余计算，导致的结果是当数据量大的时候，每个参数的更新都会很慢。
2. 随机梯度下降是以高方差频繁更新，优点是使得sgd会跳到新的和潜在更好的局部最优解，缺点是使得收敛到局部最优解的过程更加的复杂。
3. 小批量梯度下降结合了sgd和batch gd的优点，每次更新的时候使用n个样本。减少了参数更新的次数，可以达到更加稳定收敛结果，一般在深度学习当中我们采用这种方法。

## 4、逻辑回归如何分类
逻辑回归的划定一个阈值，y值大于这个阈值的是一类，y值小于这个阈值的是另外一类。阈值具体如何调整根据实际情况选择。一般会选择0.5做为阈值来划分。

## 扩展

### 1、平方损失与交叉熵损失
- 平方损失函数是线性回归假设样本误差服从高斯分布，利用极大似然估计推导而得；交叉熵损失函数是逻辑回归假设样本服从二项分布，利用极大似然估计推导而得。这是**由于模型优化目标不同而推导得到不同的损失函数**。
- 对于逻辑回归而言，**交叉熵损失函数是凸函数，而平方损失函数不是凸函数**。凸函数有着良好的优化性质，可以保证局部最优就是全局最优。而非凸函数，存在很多局部极小值，不宜优化。
- $\theta _j=\theta _j-\left ( y^{i} -h_\theta (x^{i};\theta ) \right )\ast x^{i}_j$极大似然损失训练速度较快且稳定，平方损失梯度<=0.25.

### 2、优缺点
#### 2.1、优点
1. 模型效果，工程上可接受。一般用作baseline
2. 形式简单，模型的可解释性非常好。从特征的权重可以看到不同的特征对最后结果的影响，
3. 训练速度较快。分类的时候，计算量仅仅只和特征的数目相关。
#### 2.2、缺点
1. 准确率并不是很高。因为形式非常的简单(非常类似线性模型)，很难去拟合数据的真实分布。
2. 很难处理数据不平衡的问题。举个例子：如果我们对于一个正负样本非常不平衡的问题比如正负样本比 10000:1.我们把所有样本都预测为正也能使损失函数的值比较小。但是作为一个分类器，它对正负样本的区分能力不会很好。（focal loss损失函数）
3. 处理非线性数据较麻烦。逻辑回归在不引入其他方法的情况下，只能处理线性可分的数据，或者进一步说，处理二分类的问题 。
4. 逻辑回归本身无法筛选特征。有时候，我们会用gbdt来筛选特征，然后再上逻辑回归。