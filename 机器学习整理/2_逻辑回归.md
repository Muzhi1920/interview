<!-- TOC -->

1. [逻辑回归](#逻辑回归)
   1. [逻辑回归的假设](#逻辑回归的假设)
   2. [逻辑回归的损失函数](#逻辑回归的损失函数)
   3. [逻辑回归的求解方法](#逻辑回归的求解方法)
   4. [逻辑回归如何分类](#逻辑回归如何分类)
   5. [扩展](#扩展)
      1. [平方损失与交叉熵损失](#平方损失与交叉熵损失)
      2. [优缺点](#优缺点)
         1. [优点](#优点)
         2. [缺点](#缺点)

<!-- /TOC -->

<a id="markdown-逻辑回归" name="逻辑回归"></a>
# 逻辑回归

[逻辑回归面试整理](https://www.cnblogs.com/ModifyRong/p/7739955.html)

逻辑回归假设数据服从伯努利分布,通过极大化似然函数的方法，运用梯度下降来求解参数，来达到将数据二分类的目的

**求解过程：按照极大似然估计求解或者交叉熵损失求解殊途同归**

何为极大似然：“模型已定，参数未知”；**利用已知的样本结果，反推最有可能（最大概率）导致这样结果的参数值。**

---
<a id="markdown-逻辑回归的假设" name="逻辑回归的假设"></a>
## 逻辑回归的假设
逻辑回归的第一个基本假设是假设数据服从伯努利分布
$$h_\theta\left(x;\theta \right )=\frac{1}{1+e^{-\theta^{T} x}}$$


<a id="markdown-逻辑回归的损失函数" name="逻辑回归的损失函数"></a>
## 逻辑回归的损失函数
>>样本总体的后验概率

$$L_\theta\left(x\right )= \prod _{i=1}^{m}h_\theta(x_{i};\theta )^{y_{i}}*(1-h_\theta(x_{i};\theta))^{1-y_{i}}$$
>>损失函数为负对数损失时求和

$$J(\theta)=-\frac{1}{N}\sum_{i=1}^{N}[y_{i}log(h_{\theta}(x_i))+(1-y_{i})log(1-h_{\theta}(x_i))]$$

<a id="markdown-逻辑回归的求解方法" name="逻辑回归的求解方法"></a>
## 逻辑回归的求解方法
对应着三种优化方法：随机梯度下降，批梯度下降，small batch 梯度下降三种方式：
1. 批梯度下降：会获得全局最优解，缺点是在更新每个参数的时候需要遍历所有的数据，当数据量大的时候，每个参数的更新都会很慢。
2. 随机梯度下降：以高方差频繁更新，优点是使得sgd会跳到新的和潜在更好的局部最优解，缺点是使得收敛到局部最优解的过程更加的复杂。
3. 小批量梯度下降：结合了sgd和batch gd的优点，每次更新的时候使用n个样本。减少了参数更新的次数，可以达到更加稳定收敛结果。

<a id="markdown-逻辑回归如何分类" name="逻辑回归如何分类"></a>
## 逻辑回归如何分类
逻辑回归的划定一个阈值，y值大于这个阈值的是一类，y值小于这个阈值的是另外一类。阈值具体如何调整根据实际情况选择。一般会选择0.5做为阈值来划分。

<a id="markdown-扩展" name="扩展"></a>
## 扩展

<a id="markdown-平方损失与交叉熵损失" name="平方损失与交叉熵损失"></a>
### 平方损失与交叉熵损失
- 平方损失函数是线性回归假设样本误差服从高斯分布，利用极大似然估计推导而得；交叉熵损失函数是逻辑回归假设样本服从二项分布，利用极大似然估计推导而得。这是**由于模型优化目标不同而推导得到不同的损失函数**。
- $\theta _j=\theta _j-\left ( y^{i} -h_\theta (x^{i};\theta ) \right )\ast x^{i}_j$交叉熵损失，误差较大时训练速度较快且稳定，平方损失梯度<=0.25.

<a id="markdown-优缺点" name="优缺点"></a>
### 优缺点
<a id="markdown-优点" name="优点"></a>
#### 优点
1. 模型效果，工程上可接受。一般用作baseline
2. 形式简单，模型的可解释性非常好。从特征的权重可以看到不同的特征对最后结果的影响，
3. 训练速度较快。分类的时候，计算量仅仅只和特征的数目相关。
<a id="markdown-缺点" name="缺点"></a>
#### 缺点
1. 准确率并不是很高。因为形式非常的简单(非常类似线性模型)，很难去拟合数据的真实分布。
2. 处理非线性数据较麻烦。逻辑回归在不引入其他方法的情况下，只能处理线性可分的数据，或者进一步说，处理二分类的问题 。
3. 逻辑回归本身无法筛选特征。有时候，我们会用gbdt来筛选特征，然后再上逻辑回归。