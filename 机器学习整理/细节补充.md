<!-- TOC -->

- [Faiss](#faiss)
  - [1. 索引Index](#1-索引index)
  - [2. PCA降维](#2-pca降维)
  - [3、Product quantization(乘积量化)](#3product-quantization乘积量化)
- [Transformer+DNN](#transformerdnn)

<!-- /TOC -->

<a id="markdown-faiss" name="faiss"></a>
## Faiss
>>支持两种相似性计算方法：L2距离（即欧式距离）和点乘（归一化的向量点乘即cosine相似度）
<a id="markdown-1-索引index" name="1-索引index"></a>
### 1. 索引Index
<a id="markdown-2-pca降维" name="2-pca降维"></a>
### 2. PCA降维
PCA数据降维减少内存，加快机器学习的速度。从数据存储的角度，图片处理中通过PCA可以将图片从高维空间（p维）转换到低维空间（q维, 其中p > q ），其具体操作便是是将高维空间中的图片向量（n\*p）乘以一个转换矩阵M（p\*q），得到一个低维空间中的向量（n*q）；转换矩阵M是与图库有关，是可以由图库数据计算出来的。在Faiss的一些预处理中，我们会引入一些参数，这些参数又无法一开始由人工来指定，只能通过喂样本来训练出来。
<a id="markdown-3product-quantization乘积量化" name="3product-quantization乘积量化"></a>
### 3、Product quantization(乘积量化)
 PQ算法可以理解为首先把原始的向量空间分解为m个低维向量空间的笛卡尔积，并对分解得到的低维向量空间分别做量化。即是把原始D维向量（比如D=128）分成m组（比如m=4），每组就是D∗=D/m维的子向量（比如D∗=D/m=128/4=32），各自用kmeans算法学习到一个码本，然后这些码本的笛卡尔积就是原始D维向量对应的码本。用qj表示第j组子向量，用Cj表示其对应学习到的码本，那么原始D维向量对应的码本就是C=C1×C2×…×Cm。用k∗表示子向量的聚类中心点数或者说码本大小，那么原始D维向量对应的聚类中心点数或者说码本大小就是k=(k∗)m。
![](https://i.typcdn.com/fabwrite/c5/L56k1gZ7ZumryJrAfzzw.png)

SDC算法：先用PQ量化器对x和y表示为对应的中心点q(x)和q(y)，然后用公式1来近似d(x,y)。这里 q 表示 PQ量化过程。
$$\hat{d}(x,y)=d(q(x),q(y))=\sqrt{\sum_j{d(q_j(x),q_j(y))^2}}$$
ADC算法：只对y表示为对应的中心点q(y)，然后用下述公式2来近似d(x,y)。
$$\widetilde{d}(x,y)=d(x,q(y))=\sqrt{\sum_j{d(u_j(x),q_j(u_j(y)))^2}}$$


<a id="markdown-transformerdnn" name="transformerdnn"></a>
## Transformer+DNN
1、偏差较大：精排得分，确定下限；
2、测试机较差时怀疑过拟合，dropout后测试集效果好于训练评估；
3、测试集不够具有全天代表性，扩充；
4、最终保证测试与训练相差不大。


