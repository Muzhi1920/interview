<a id="markdown-深度学习" name="深度学习"></a>
# 深度学习
<!-- TOC -->

- [深度学习](#深度学习)
  - [一、激活函数](#一激活函数)
    - [0、激活函数选择](#0激活函数选择)
    - [1. 为什么非线性激活函数](#1-为什么非线性激活函数)
    - [2. ReLU 相比 sigmoid 的优势](#2-relu-相比-sigmoid-的优势)
  - [二、正则化](#二正则化)
    - [1、批归一化](#1批归一化)
    - [2、L1和L2正则](#2l1和l2正则)
      - [L1、L2区别](#l1l2区别)
    - [3、Dropout](#3dropout)
    - [4、参数初始化](#4参数初始化)

<!-- /TOC -->

<a id="markdown-一激活函数" name="一激活函数"></a>
## 一、激活函数
<a id="markdown-0激活函数选择" name="0激活函数选择"></a>
### 0、激活函数选择
- 分类，请使用softmax作为最后一层的非线性和交叉熵作为成本函数。
- 回归，则将sigmoid或tanh用作最后一层的非线性和平方误差作为成本函数。
- 使用更好的优化器(AdamOptimizer，AdagradOptimizer)而不是GradientDescentOptimizer，或者使用动量来更快地收敛，
- 使用ReLU作为层之间的非线性。另一个版本的ReLU，叫Leaky ReLU。当z<0时，斜率非常平缓，一般表达式为a=max(0.01z, z）。
- σ-函数一般用在输出结果为二分类的输出层。一般隐藏层选用tanh函数或是ReLU，最常用的是ReLU，具有梯度下降速度快的优点
- ReLU虽好，但也存在当z为负时，导数为0的小缺点，虽然这在实践过程中并不会带来什么问题，但也可以用Leaky ReLU达到更好的效果，虽然目前Leaky ReLU还是比较少用。

<a id="markdown-1-为什么非线性激活函数" name="1-为什么非线性激活函数"></a>
### 1. 为什么非线性激活函数
- **神经网络万能近似定理**认为神经网络具有至少一个非线性隐藏层，那么只要给予网络足够数量的隐藏单元，**隐层就可以以任意的精度来拟合函数：完成任何一个有限维空间到另一个有限维空间的函数。**
- 如果不使用非线性激活函数，那么每一层输出都是上层输入的线性组合，整体也是线性的，失去万能近似的性质。
- 部分层是纯线性是可以接受的，这有助于减少网络中的参数。

<a id="markdown-2-relu-相比-sigmoid-的优势" name="2-relu-相比-sigmoid-的优势"></a>
### 2. ReLU 相比 sigmoid 的优势
1. 避免梯度消失***
   1. sigmoid函数在输入取绝对值非常大的正值或负值时输出值饱和，此时导数≈0从而造成梯度消失；
   2. ReLU 的导数始终是常数，所以不会发生梯度消失现象；
2. 减缓过拟合**
   1. ReLU 在负半区的输出为0,不会产生梯度和被训练，使得网络稀疏——稀疏激活，减少参数的相互依赖，缓解过拟合问题的发生；
3. 加速计算
   1. ReLU 的求导不涉及浮点运算，所以速度更快；
>>非全局可导:工程实现，不报错即可

<a id="markdown-二正则化" name="二正则化"></a>
## 二、正则化
<a id="markdown-1批归一化" name="1批归一化"></a>
### 1、批归一化
- 什么是批归一化
  - 公式![avatar](https://github.com/imhuay/Algorithm_Interview_Notes-Chinese/raw/master/_assets/%E5%85%AC%E5%BC%8F_20180903224323.png)
  - 每一批数据层层传播，致使网络在每次迭代中都需要拟合不同的数据分布，增大了网络的训练难度与过拟合的风险。对某层的神经元进行归一化（标准正态）处理，使其限制在统一分布下。相当于**对数据分布进行额外的约束，降低拟合能力，破坏已学到的特征分布，从而增强模型的泛化能力。**
- 如何恢复？
  - 公式![avatar](https://github.com/imhuay/Algorithm_Interview_Notes-Chinese/raw/master/_assets/%E5%85%AC%E5%BC%8F_20180831165516.png)
- 批归一化的作用
    1. 加速网络的训练（缓解梯度消失，支持更大的学习率）
    2. 防止过拟合
    3. 降低了参数初始化的要求。

<a id="markdown-2l1和l2正则" name="2l1和l2正则"></a>
### 2、L1和L2正则
<a id="markdown-l1l2区别" name="l1l2区别"></a>
#### L1、L2区别
- 相同点：通过限制参数的规模，使模型偏好于权值较小的目标函数，防止过拟合。
- 不同点
    1. L1 正则化可以产生更稀疏的权值矩阵，可以用于特征选择，同时一定程度上防止过拟合；L1 正则化适用于特征之间有关联的情况。
    2. L2 正则化整体降低模型权重，模型复杂度更低，模型对于噪声与异常点的抗干扰性的能力增强，从而提高模型的泛化能力。

<a id="markdown-3dropout" name="3dropout"></a>
### 3、Dropout
>>训练时丢失，预测时都加上

隐藏层的采样概率为 0.5，输入的采样概率为 0.8；
![avatar](https://github.com/imhuay/Algorithm_Interview_Notes-Chinese/raw/master/_assets/TIM%E6%88%AA%E5%9B%BE20180611152559.png)

<a id="markdown-4参数初始化" name="4参数初始化"></a>
### 4、参数初始化
1. 一般使用服从的高斯分布（mean=0, stddev=1）或均匀分布的随机值作为权重的初始化参数；使用 0 作为偏置的初始化参数
2. 一些启发式方法会根据输入与输出的单元数来决定初始值的范围；比如
![avatar](https://github.com/imhuay/Algorithm_Interview_Notes-Chinese/raw/master/_assets/%E5%85%AC%E5%BC%8F_20180706115540.png)
Keras 全连接层默认的权重初始化方法

<a id="markdown-三优化算法" name="三优化算法"></a>
