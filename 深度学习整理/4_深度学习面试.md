# 深度学习
<!-- TOC -->

- [深度学习](#深度学习)
    - [激活函数](#激活函数)
        - [如何使用激活函数](#如何使用激活函数)
        - [为什么非线性激活函数](#为什么非线性激活函数)
        - [各种激活函数优缺点](#各种激活函数优缺点)
    - [二、正则化](#二正则化)
        - [1、批归一化](#1批归一化)
        - [2、L1和L2正则](#2l1和l2正则)
            - [L1、L2区别](#l1l2区别)
        - [3、Dropout](#3dropout)
        - [4、参数初始化](#4参数初始化)
    - [神经网络的偏置项b](#神经网络的偏置项b)
    - [NN的参数初始化](#nn的参数初始化)
    - [交叉熵损失与平方差损失](#交叉熵损失与平方差损失)
    - [神经网络和SVM的区别](#神经网络和svm的区别)
    - [神经网络的宽度和深度](#神经网络的宽度和深度)
    - [神经网络万能近似定理](#神经网络万能近似定理)

<!-- /TOC -->

## 激活函数
### 如何使用激活函数
- 回归，则将sigmoid或tanh用作最后一层的非线性和平方误差作为成本函数。
- 分类，隐层relu+尾层sigmoid+softmax归一化输出+结果交叉熵作损失

### 为什么非线性激活函数
- **神经网络万能近似定理**认为神经网络具有至少一个非线性隐藏层，那么只要给予网络足够数量的隐藏单元，**隐层就可以以任意的精度来拟合函数：完成任何一个有限维空间到另一个有限维空间的函数。**
- 如果不使用非线性激活函数，那么每一层输出都是上层输入的线性组合，整体也是线性的，失去万能近似的性质。
- 部分层是纯线性是可以接受的，这有助于减少网络中的参数。

### 各种激活函数优缺点
- $sigmoid$
  1. 优点：较好的解释性；
  2. 缺点：函数饱和梯度消失；非0中心，梯度恒为正或负；指数运算慢；
- $tanh$
  1. 优点：0中心；
  2. 缺点：饱和梯度消失，计算慢；
- $ReLu$
  1. 优点：加速梯度下降收敛；线性非饱和；特别适用DNN；
  2. 缺点：负半区失活；稀疏；


## 二、正则化
### 1、批归一化
- 什么是批归一化
  - 公式![avatar](https://github.com/imhuay/Algorithm_Interview_Notes-Chinese/raw/master/_assets/%E5%85%AC%E5%BC%8F_20180903224323.png)
  - 每一批数据层层传播，致使网络在每次迭代中都需要拟合不同的数据分布，增大了网络的训练难度与过拟合的风险。对某层的神经元进行归一化（标准正态）处理，使其限制在统一分布下。相当于**对数据分布进行额外的约束，降低拟合能力，破坏已学到的特征分布，从而增强模型的泛化能力。**
- 如何恢复？
  - 公式![avatar](https://github.com/imhuay/Algorithm_Interview_Notes-Chinese/raw/master/_assets/%E5%85%AC%E5%BC%8F_20180831165516.png)
- 批归一化的作用
    1. 加速网络的训练（缓解梯度消失，支持更大的学习率）
    2. 防止过拟合
    3. 降低了参数初始化的要求。

### 2、L1和L2正则
#### L1、L2区别
- 相同点：通过限制参数的规模，使模型偏好于权值较小的目标函数，防止过拟合。
- 不同点
    1. L1 正则化可以产生更稀疏的权值矩阵，可以用于特征选择，同时一定程度上防止过拟合；L1 正则化适用于特征之间有关联的情况。
    2. L2 正则化整体降低模型权重，模型复杂度更低，模型对于噪声与异常点的抗干扰性的能力增强，从而提高模型的泛化能力。

### 3、Dropout
>>训练时丢失，预测时都加上

隐藏层的采样概率为 0.5，输入的采样概率为 0.8；
![avatar](https://github.com/imhuay/Algorithm_Interview_Notes-Chinese/raw/master/_assets/TIM%E6%88%AA%E5%9B%BE20180611152559.png)

### 4、参数初始化
1. 一般使用服从的高斯分布（mean=0, stddev=1）或均匀分布的随机值作为权重的初始化参数；使用 0 作为偏置的初始化参数
2. 一些启发式方法会根据输入与输出的单元数来决定初始值的范围；比如
![avatar](https://github.com/imhuay/Algorithm_Interview_Notes-Chinese/raw/master/_assets/%E5%85%AC%E5%BC%8F_20180706115540.png)
Keras 全连接层默认的权重初始化方法

## 神经网络的偏置项b
$$\hat y=\sum_i w_i x_i$$
$$y=1,if\hat{y}>= threshold$$
$$y=0,if\hat{y}<= threshold$$
$threshold$度量神经元正负激励的难易程度，$threshold$越大产生正向激励难度越大。通过移项作为$-b$加入最终的值中判断是否大于0即可。所以**$b$的大小，度量了神经元产生正（负）激励的难易程度**

## NN的参数初始化
如果无隐层则参数可以都初始化为0；其他不会使用0初始化；

## 交叉熵损失与平方差损失
>>对分类问题
- 交叉熵度量两个分布的相似性；平方差损失度量两个数据距离；
- 交叉熵对概率更为敏感；平方差对数值敏感；
- 分类问题最后是sigmoid的输出，交叉熵可以使误差越大时学习率越大收敛快；平方差损失学习则慢；

平方损失用在输出连续，最后一层无sigmoid和softmax层的NN中。



## 神经网络和SVM的区别
SVM和NN都可实现非线性分类模型。
- NN通过多个隐层实现非线性函数-**神经网络万能近似定理**。设计灵活，但可解释性较差；
- VM使用核函数的方法，实现一个泛函线性空间。核函数设计复杂；可解释性较强，理论完备；

## 神经网络的宽度和深度
宽度设计：参数过多；
深度设计：参数较少。

## 神经网络万能近似定理
一个前馈神经网络如果具有线性层和至少一个“挤压”性质的激活函数，给定网络足够的隐层，它可以以任意精度近似一个函数，实现一个有限维空间到另一个有限维空间的映射。






