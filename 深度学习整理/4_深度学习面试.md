## 一、激活函数
### 0、激活函数选择
- 分类，请使用softmax作为最后一层的非线性和交叉熵作为成本函数。
- 回归，则将sigmoid或tanh用作最后一层的非线性和平方误差作为成本函数。
- 使用更好的优化器(AdamOptimizer，AdagradOptimizer)而不是GradientDescentOptimizer，或者使用动量来更快地收敛，
- 使用ReLU作为层之间的非线性。另一个版本的ReLU，叫Leaky ReLU。当z<0时，斜率非常平缓，一般表达式为a=max(0.01z, z）。
- σ-函数一般用在输出结果为二分类的输出层。一般隐藏层选用tanh函数或是ReLU，最常用的是ReLU，具有梯度下降速度快的优点
- ReLU虽好，但也存在当z为负时，导数为0的小缺点，虽然这在实践过程中并不会带来什么问题，但也可以用Leaky ReLU达到更好的效果，虽然目前Leaky ReLU还是比较少用。

### 1. 为什么非线性激活函数
- **神经网络万能近似定理**认为神经网络具有至少一个非线性隐藏层，那么只要给予网络足够数量的隐藏单元，**隐层就可以以任意的精度来拟合函数：完成任何一个有限维空间到另一个有限维空间的函数。**
- 如果不使用非线性激活函数，那么每一层输出都是上层输入的线性组合，整体也是线性的，失去万能近似的性质。
- 部分层是纯线性是可以接受的，这有助于减少网络中的参数。

### 2. ReLU 相比 sigmoid 的优势
1. 避免梯度消失***
   1. sigmoid函数在输入取绝对值非常大的正值或负值时输出值饱和，此时导数≈0从而造成梯度消失；
   2. ReLU 的导数始终是常数，所以不会发生梯度消失现象；
2. 减缓过拟合**
ReLU 在负半区的输出为0,不会产生梯度和被训练，使得网络稀疏——稀疏激活，减少参数的相互依赖，缓解过拟合问题的发生；
3. 加速计算
ReLU 的求导不涉及浮点运算，所以速度更快；
>>非全局可导:工程实现，不报错即可

## 二、正则化
### 1、批归一化
- 什么是批归一化
  - 公式![avatar](https://github.com/imhuay/Algorithm_Interview_Notes-Chinese/raw/master/_assets/%E5%85%AC%E5%BC%8F_20180903224323.png)
  - 每一批数据层层传播，致使网络在每次迭代中都需要拟合不同的数据分布，增大了网络的训练难度与过拟合的风险。对某层的神经元进行归一化（标准正态）处理，使其限制在统一分布下。相当于对数据分布进行额外的约束，降低拟合能力，破坏已学到的特征分布，从而增强模型的泛化能力，
- 如何恢复？
  - 公式![avatar](https://github.com/imhuay/Algorithm_Interview_Notes-Chinese/raw/master/_assets/%E5%85%AC%E5%BC%8F_20180831165516.png)
- 批归一化的作用
    1. 加速网络的训练（缓解梯度消失，支持更大的学习率）
    2. 防止过拟合
    3. 降低了参数初始化的要求。

### 2、L1和L2正则
- 模型权值更小，意味着更低的模型复杂度；添加 L1 & L2 正则化相当于为模型添加了某种先验，限制了参数的分布，从而降低了模型的复杂度。
- 模型的复杂度降低，意味着模型对于噪声与异常点的抗干扰性的能力增强，从而提高模型的泛化能力。

#### L1、L2区别
- 相同点：通过限制参数的规模，使模型偏好于权值较小的目标函数，防止过拟合。
- 不同点
    1. L1 正则化可以产生更稀疏的权值矩阵，可以用于特征选择，同时一定程度上防止过拟合；L1 正则化适用于特征之间有关联的情况
    2. L2 正则化主要用于防止模型过拟合，适用于特征之间没有关联的情况

### 3、Dropout
>>训练时丢失，预测时都加上

隐藏层的采样概率为 0.5，输入的采样概率为 0.8；
![avatar](https://github.com/imhuay/Algorithm_Interview_Notes-Chinese/raw/master/_assets/TIM%E6%88%AA%E5%9B%BE20180611152559.png)

### 4、参数初始化
1. 一般使用服从的高斯分布（mean=0, stddev=1）或均匀分布的随机值作为权重的初始化参数；使用 0 作为偏置的初始化参数
2. 一些启发式方法会根据输入与输出的单元数来决定初始值的范围；比如
![avatar](https://github.com/imhuay/Algorithm_Interview_Notes-Chinese/raw/master/_assets/%E5%85%AC%E5%BC%8F_20180706115540.png)
Keras 全连接层默认的权重初始化方法

## 三、优化算法

### 3.1、梯度下降
- 梯度下降是一种**优化算法**，通过**迭代**的方式寻找模型的**最优参数**；
- 所谓最优参数指的是使**目标函数**达到最小值时的参数；
- 当目标函数是**凸函数**时，梯度下降的解是全局最优解；但在一般情况下，**梯度下降无法保证全局最优**。
- 微积分中使用**梯度**表示函数增长最快的方向；因此，神经网络中使用**负梯度**来指示目标函数下降最快的方向。
- **梯度**实际上是损失函数对网络中每个参数的**偏导**所组成的向量；
- **梯度**仅仅指示了对于每个参数各自增长最快的方向；因此，梯度无法保证**全局方向**就是函数为了达到最小值应该前进的方向。
- **梯度**的具体计算方法即**反向传播**。
- **负梯度**中的每一项可以认为传达了**两个信息**：
- 正负号在告诉输入向量应该调大还是调小（正调大，负调小）
- 每一项的相对大小表明每个参数对函数值达到最值的**影响程度**；

#### 3.1.1、随机梯度下降
- 基本的梯度下降法每次使用**所有训练样本**的**平均损失**来更新参数；
- 因此，经典的梯度下降在每次对模型参数进行更新时，需要遍历所有数据；
- 当训练样本的数量很大时，这需要消耗相当大的计算资源，在实际应用中基本不可行。
- **随机梯度下降**（SGD）每次使用单个样本的损失来近似平均损失

#### 3.1.2、小批量随机梯度下降
- 为了降低随机梯度的**方差**，使模型迭代更加稳定，实践中会使用**一批**随机数据的损失来近似平均损失。
- 使用批训练的另一个主要目的，是为了利用高度优化的**矩阵运算**以及**并行计算框架**。

#### 3.1.3、“批”的大小对优化效果的影响
- **较大的批能得到更精确的梯度估计**，但回报是小于线性的。
- **较小的批能带来更好的泛化误差**，泛化误差通常在批大小为 1 时最好。
- 原因可能是由于小批量在学习过程中带来了**噪声**，使产生了一些正则化效果 (Wilson and Martinez, 2003)
- 但是，因为梯度估计的高方差，小批量训练需要**较小的学习率**以保持稳定性，这意味着**更长的训练时间**。
- 当批的大小为 **2 的幂**时能充分利用矩阵运算操作，所以批的大小一般取 32、64、128、256 等。
- **内存消耗和批的大小成正比**，当批量处理中的所有样本可以并行处理时。
- 在某些硬件上使用特定大小可以减少运行时间。尤其是在使用 GPU 时，通常使用 **2 的幂数**作为批量大小可以获得更少的运行时间。一般，2 的幂数的**取值范围是 32 到 256**，16 有时在尝试大模型时使用。

#### 3.1.4、随机梯度下降存在的问题
- 随机梯度下降（SGD）放弃了**梯度的准确性**，仅采用一部分样本来估计当前的梯度；因此 SGD 对梯度的估计常常出现偏差，造成目标函数收敛不稳定，甚至不收敛的情况。
- 无论是经典的梯度下降还是随机梯度下降，都可能陷入**局部极值点**；除此之外，SGD 还可能遇到“**峡谷**”和“**鞍点**”两种情况
- **峡谷**类似一个带有**坡度**的狭长小道，左右两侧是“**峭壁**”；在**峡谷**中，准确的梯度方向应该沿着坡的方向向下，但粗糙的梯度估计使其稍有偏离就撞向两侧的峭壁，然后在两个峭壁间来回**震荡**。
- **鞍点**的形状类似一个马鞍，一个方向两头翘，一个方向两头垂，而**中间区域近似平地**；一旦优化的过程中不慎落入鞍点，优化很可能就会停滞下来。
![avatar](https://github.com/imhuay/Algorithm_Interview_Notes-Chinese/raw/master/_assets/TIM%E6%88%AA%E5%9B%BE20180826113534.png)


### 3.2、随机梯度下降改进
- SGD 的改进遵循两个方向：**惯性保持**和**环境感知**
- **惯性保持**指的是加入**动量** SGD 算法；**针对鞍点**
- **环境感知**指的是根据不同参数的一些**经验性判断**，**自适应**地确定**每个参数的学习速率**；**针对峡谷**

**训练词向量的例子**
- 不同词出现的频率是不同的，**数据的稀疏性会影响其参数的稀疏性**；
- 具体来说，**对低频词如果不加措施**，其参数的梯度在多数情况下为 0；换言之，这些参数更新的频率很低，导致难以收敛。
- 在实践中，我们希望学习**低频词**的参数时具有**较大的学习率**，而高频词其参数的更新幅度可以小一些。

### 3.3、动量（Momentum）算法

#### 3.3.1 带动量的 SGD
- 引入**动量**（Momentum）方法一方面是为了解决“峡谷”和“鞍点”问题；一方面也可以用于SGD 加速，特别是针对**高曲率**、小幅但是方向一致的梯度。
- 如果把原始的 SGD 想象成一个**纸团**在重力作用向下滚动，由于**质量小**受到山壁弹力的干扰大，导致来回震荡；或者在鞍点处因为**质量小**速度很快减为 0，导致无法离开这块平地。
- **动量**方法相当于把纸团换成了**铁球**；不容易受到外力的干扰，轨迹更加稳定；同时因为在鞍点处因为**惯性**的作用，更有可能离开平地。
- 动量方法以一种廉价的方式模拟了二阶梯度（牛顿法）
![avatar](https://github.com/imhuay/Algorithm_Interview_Notes-Chinese/raw/master/_assets/TIM%E6%88%AA%E5%9B%BE20180819110834.png)

- **参数更新公式**
![avatar](https://github.com/imhuay/Algorithm_Interview_Notes-Chinese/raw/master/_assets/%E5%85%AC%E5%BC%8F_20180826203915.png)
- 从形式上看， 动量算法引入了变量 `v` 充当速度角色，以及相相关的超参数 `α`。
- 原始 SGD 每次更新的步长只是梯度乘以学习率；现在，步长还取决于**历史梯度序列**的大小和排列；当许多连续的梯度指向**相同的方向**时，步长会被不断增大；

- **动量算法描述**
![avatar](https://github.com/imhuay/Algorithm_Interview_Notes-Chinese/raw/master/_assets/TIM%E6%88%AA%E5%9B%BE20180611203427.png)
- 如果动量算法总是观测到梯度 `g`，那么它会在 `−g` 方向上不断加速，直到达到**最终速度**。
![avatar](https://github.com/imhuay/Algorithm_Interview_Notes-Chinese/raw/master/_assets/%E5%85%AC%E5%BC%8F_20180826161333.png)

- 在实践中， `α` 的一般取 `0.5, 0.9, 0.99`，分别对应**最大** `2` 倍、`10` 倍、`100` 倍的步长
- 和学习率一样，`α` 也可以使用某种策略在训练时进行**自适应调整**；一般初始值是一个较小的值，随后会慢慢变大。

#### 3.3.2 NAG 算法（Nesterov 动量）
- **NAG 把梯度计算放在对参数施加当前速度之后**。
- 这个“**提前量**”的设计让算法有了对前方环境“**预判**”的能力。Nesterov 动量可以解释为往标准动量方法中添加了一个**修正因子**。
- **NAG 算法描述**
![avatar](https://github.com/imhuay/Algorithm_Interview_Notes-Chinese/raw/master/_assets/TIM%E6%88%AA%E5%9B%BE20180611211753.png)

#### 3.3.3 区别
- momentum：先计算梯度，再加上动量加速$\alpha*v$
- nesterov：先加上动量加速$\alpha*v$,再去计算梯度；

### 3.4、自适应学习率的优化算法

#### 3.4.1、AdaGrad
- 该算法的思想是独立地适应模型的每个参数：具有较大偏导的参数相应有一个较大的学习率，而具有小偏导的参数则对应一个较小的学习率
- 具体来说，每个参数的学习率会缩放各参数反比于其**历史梯度平方值总和的平方根**
- **AdaGrad 算法描述**
![avatar](https://github.com/imhuay/Algorithm_Interview_Notes-Chinese/raw/master/_assets/TIM%E6%88%AA%E5%9B%BE20180611214508.png)
- 注意：全局学习率 `ϵ` 并没有更新，而是每次应用时被缩放

**AdaGrad 存在的问题**
- 学习率是单调递减的，训练后期学习率过小会导致训练困难，甚至提前结束
- 需要设置一个全局的初始学习率

#### 3.4.2、RMSProp
- RMSProp 主要是为了解决 AdaGrad 方法中**学习率过度衰减**的问题—— AdaGrad 根据平方梯度的**整个历史**来收缩学习率，可能使得学习率在达到局部最小值之前就变得太小而难以继续训练；
- RMSProp 使用**指数衰减平均**（递归定义）以丢弃遥远的历史，使其能够在找到某个“凸”结构后快速收敛；此外，RMSProp 还加入了一个超参数 `ρ` 用于控制衰减速率。
- 具体来说（对比 AdaGrad 的算法描述），即修改 `r` 为
![avatar](https://github.com/imhuay/Algorithm_Interview_Notes-Chinese/raw/master/_assets/%E5%85%AC%E5%BC%8F_20180819204052.png)

记![avatar](https://github.com/imhuay/Algorithm_Interview_Notes-Chinese/raw/master/_assets/%E5%85%AC%E5%BC%8F_20180819204052.png)

则![avatar](https://github.com/imhuay/Algorithm_Interview_Notes-Chinese/raw/master/_assets/%E5%85%AC%E5%BC%8F_20180819204612.png)
> 其中 `E` 表示期望，即平均；`δ` 为平滑项，具体为一个小常数，一般取 `1e-8 ~ 1e-10`（Tensorflow 中的默认值为 `1e-10`）
- **RMSProp** 建议的**初始值**：全局学习率 `ϵ=1e-3`，衰减速率 `ρ=0.9`
- **RMSProp 算法描述**
![avatar](https://github.com/imhuay/Algorithm_Interview_Notes-Chinese/raw/master/_assets/TIM%E6%88%AA%E5%9B%BE20180611215422.png)

- 带 **Nesterov 动量**的 **RMSProp**
![avatar](https://github.com/imhuay/Algorithm_Interview_Notes-Chinese/raw/master/_assets/TIM%E6%88%AA%E5%9B%BE20180611215923.png)

- 经验上，RMSProp 已被证明是一种有效且实用的深度神经网络优化算法。
- RMSProp 依然需要设置一个全局学习率，同时又多了一个超参数（推荐了默认值）。

#### 3.4.3、AdaDelta
- AdaDelta 和 RMSProp 都是为了解决 AdaGrad 对学习率过度衰减的问题而产生的。
- AdaDelta 和 RMSProp 是独立发现的，AdaDelta 的前半部分与 RMSProp 完全一致；
- AdaDelta 进一步解决了 AdaGrad 需要设置一个全局学习率的问题
- 具体来说，即
![avatar](https://github.com/imhuay/Algorithm_Interview_Notes-Chinese/raw/master/_assets/%E5%85%AC%E5%BC%8F_20180819203034.png)
此时，AdaDelta 已经不需要设置全局学习率了


#### 3.4.4、Adam
- Adam 在 RMSProp 方法的基础上更进一步：
- 除了加入**历史梯度平方的指数衰减平均**（`r`）外，
- 还保留了**历史梯度的指数衰减平均**（`s`），相当于**动量**。
- Adam 行为就像一个带有摩擦力的小球，在误差面上倾向于平坦的极小值。

- **Adam 算法描述**
![avatar](https://github.com/imhuay/Algorithm_Interview_Notes-Chinese/raw/master/_assets/TIM%E6%88%AA%E5%9B%BE20180611220109.png)

**偏差修正**
- 注意到，`s` 和 `r` 需要初始化为 `0`；且 `ρ1` 和 `ρ2` 推荐的初始值都很接近 `1`（`0.9` 和 `0.999`）
- 这将导致在训练初期 `s` 和 `r` 都很小（偏向于 0），从而训练缓慢。
- 因此，Adam 通过修正偏差来抵消这个倾向。

![avatar](https://img-blog.csdn.net/20180114121415460?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMjgwMzE1MjU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

#### 3.4.5、AdaMax
- Adam 的一个变种，对梯度平方的处理由**指数衰减平均**改为**指数衰减求最大值**

#### 3.4.6、Nadam
- Nesterov 动量版本的 Adam
#### 区别
![avatar](https://img-blog.csdn.net/20180703200322455?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzIzMjY5NzYx/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

## 四、如何选择这些优化算法？
- 各自适应学习率的优化算法表现不分伯仲，没有哪个算法能在所有任务上脱颖而出；
- 目前，最流行并且使用很高的优化算法包括 SGD、带动量的 SGD、RMSProp、带动量的 RMSProp、AdaDelta 和 Adam。
- 具体使用哪个算法取决于使用者对算法的熟悉程度，以便调节超参数。

### 各优化算法的可视化
- SGD 各优化方法在损失曲面上的表现
![avatar](https://github.com/imhuay/Algorithm_Interview_Notes-Chinese/raw/master/_assets/contours_evaluation_optimizers.gif)

- SGD 各优化方法在**鞍点**处上的表现
![avatar](https://github.com/imhuay/Algorithm_Interview_Notes-Chinese/raw/master/_assets/saddle_point_evaluation_optimizers.gif)




