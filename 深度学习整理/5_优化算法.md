<a id="markdown-优化算法" name="优化算法"></a>
# 优化算法
<!-- TOC -->

- [优化算法](#优化算法)
  - [梯度下降](#梯度下降)
    - [批梯度下降（Batch SGD） VS 随机梯度下降](#批梯度下降batch-sgd-vs-随机梯度下降)
    - [小批量随机梯度下降（mini batch SGD）](#小批量随机梯度下降mini-batch-sgd)
    - [“批”的大小对优化效果的影响](#批的大小对优化效果的影响)
    - [随机梯度下降存在的问题](#随机梯度下降存在的问题)
  - [改进的随机梯度下降](#改进的随机梯度下降)
    - [动量算法](#动量算法)
      - [Momentum动量](#momentum动量)
      - [NAG 动量](#nag-动量)
      - [Momentum和NAG动量的区别](#momentum和nag动量的区别)
    - [自适应学习率的优化算法](#自适应学习率的优化算法)
      - [AdaGrad](#adagrad)
        - [AdaGrad 算法描述](#adagrad-算法描述)
        - [AdaGrad 存在的问题](#adagrad-存在的问题)
      - [RMSProp](#rmsprop)
        - [RMSProp 算法描述](#rmsprop-算法描述)
      - [Adam](#adam)
        - [Adam 算法描述](#adam-算法描述)
  - [如何选择这些优化算法](#如何选择这些优化算法)
    - [各优化算法的可视化](#各优化算法的可视化)

<!-- /TOC -->
<a id="markdown-梯度下降" name="梯度下降"></a>
## 梯度下降
梯度下降是一种**优化算法**，通过**迭代**的方式寻找模型的**最优参数**；当目标函数是**凸函数**时，梯度下降的解是全局最优解；但在一般情况下，**梯度下降无法保证全局最优**。**负梯度**中的每一项可以认为传达了**两个信息**:
- 正负号在告诉输入向量应该调大还是调小（正调大，负调小）；
- 每一项的相对大小表明每个参数对函数值达到最值的**影响程度**

<a id="markdown-批梯度下降batch-sgd-vs--随机梯度下降" name="批梯度下降batch-sgd-vs--随机梯度下降"></a>
### 批梯度下降（Batch SGD） VS  随机梯度下降

- **批梯度下降**在每次对模型参数进行更新时，需要遍历所有数据，得到平均损失
![](https://miro.medium.com/max/569/1*Dhr8kQr0XXKSfm0Ry_YNMw.png)
- **随机梯度下降**每次使用单个样本的损失来近似平均损失
![](https://miro.medium.com/max/614/1*upQNMC4O3J1DXslT2sT7vA.png)
<a id="markdown-小批量随机梯度下降mini-batch-sgd" name="小批量随机梯度下降mini-batch-sgd"></a>
### 小批量随机梯度下降（mini batch SGD）
- 为了降低随机梯度的**方差**，使模型迭代更加稳定，实践中会使用**一批**随机数据的损失来近似平均损失。
- 使用批训练的另一个主要目的，是为了利用高度优化的**矩阵运算**以及**并行计算框架**。

<a id="markdown-批的大小对优化效果的影响" name="批的大小对优化效果的影响"></a>
### “批”的大小对优化效果的影响

- 批越小：则泛化误差更好，学习更加充分，引入噪声正则化效果，学习率小保证稳定性故收敛慢；
- 批越大则梯度估计更加精确，训练加速

>>当批的大小为 **2 的幂**时能充分利用矩阵运算操作，所以批的大小一般取 32、64、128、256 等。

<a id="markdown-随机梯度下降存在的问题" name="随机梯度下降存在的问题"></a>
### 随机梯度下降存在的问题
- 梯度下降陷入**局部极值点**，以及遇到“**峡谷**”和“**鞍点**”两种情况
- **峡谷**：准确的梯度方向应该沿着坡的方向向下，但粗糙的梯度估计导致在两个峭壁间来回震荡。
- **鞍点**：落入鞍点导致优化很可能就会停滞下来。

![avatar](https://github.com/imhuay/Algorithm_Interview_Notes-Chinese/raw/master/_assets/TIM%E6%88%AA%E5%9B%BE20180826113534.png)


<a id="markdown-改进的随机梯度下降" name="改进的随机梯度下降"></a>
## 改进的随机梯度下降
SGD 的改进遵循两个方向：**惯性保持**和**环境感知**
- **惯性保持**指的是加入动量SGD 算法；针对鞍点
- **环境感知**指的是**自适应**地确定**每个参数的学习速率**；针对峡谷


<a id="markdown-动量算法" name="动量算法"></a>
### 动量算法


<a id="markdown-momentum动量" name="momentum动量"></a>
#### Momentum动量

引入**动量**方法一方面是为了解决“峡谷”和“鞍点”问题；一方面也可以用于SGD 加速，特别是针对**高曲率**、小幅但是方向一致的梯度。在鞍点处因为**惯性**的作用，更有可能离开平地。
![avatar](https://github.com/imhuay/Algorithm_Interview_Notes-Chinese/raw/master/_assets/TIM%E6%88%AA%E5%9B%BE20180819110834.png)

>>将过去时间steps的梯度矢量累加到当前去更新梯度，致使梯度相同的方向的维度，动量项增加，加快SGD的正确方向，并抑制震荡。
$$v_t = \gamma v_{t-1} + \eta \nabla_\theta J(\theta) \\
\theta = \theta-v_t$$
缺点：梯度太大，略过最小值，在最小值附近震荡。
<a id="markdown-nag-动量" name="nag-动量"></a>
#### NAG 动量
$$v_t = \gamma v_{t-1} + \eta \nabla_\theta J(\theta - \gamma v_{t-1}) \\
\theta = \theta-v_t$$
![](https://miro.medium.com/max/1076/1*kVl-Cpxzd-LmiJ_8nu5XOg.png)
具有前瞻性，前瞻位置估计梯度

<a id="markdown-momentum和nag动量的区别" name="momentum和nag动量的区别"></a>
#### Momentum和NAG动量的区别
- momentum：计算梯度->加上动量加速$\gamma*v$->更新参数；
- nesterov：加上动量加速$\gamma*v$去计算梯度->加上动量加速$\gamma*v$->更新参数。


<a id="markdown-自适应学习率的优化算法" name="自适应学习率的优化算法"></a>
### 自适应学习率的优化算法

<a id="markdown-adagrad" name="adagrad"></a>
#### AdaGrad
- 该算法的思想是独立地适应模型的每个参数：具有较大偏导的参数相应有一个较大的学习率，而具有小偏导的参数则对应一个较小的学习率。学习率会反比于其**历史梯度平方值总和的平方根**

<a id="markdown-adagrad-算法描述" name="adagrad-算法描述"></a>
##### AdaGrad 算法描述

$$r=r+g*g$$
$$\Delta\theta=-\frac{\eta}{\sqrt{r+\delta}}*g$$
注意：全局学习率$\eta$并没有更新，而是每次应用时被缩放

<a id="markdown-adagrad-存在的问题" name="adagrad-存在的问题"></a>
##### AdaGrad 存在的问题
学习率是一直递减的，训练后期学习率过小会导致训练困难，甚至提前结束。

<a id="markdown-rmsprop" name="rmsprop"></a>
#### RMSProp
AdaGrad 根据平方梯度的**整个历史**来收缩学习率，可能使得学习率在达到局部最小值之前就变得太小而难以继续训练;RMSProp 主要是为了解决 AdaGrad 方法中**学习率过度衰减**的问题

---
RMSProp 使用**指数衰减平均**（递归定义）以丢弃遥远的历史，使其能够在找到某个“凸”结构后快速收敛；此外，RMSProp 还加入了一个超参数 `ρ` 用于控制衰减速率。

<a id="markdown-rmsprop-算法描述" name="rmsprop-算法描述"></a>
##### RMSProp 算法描述
$$r=\rho r+ (1-\rho)g*g$$
$$\Delta\theta=-\frac{\eta}{\sqrt{r+\delta}}*g$$


<a id="markdown-adam" name="adam"></a>
#### Adam
- 加入**历史梯度平方的指数衰减平均**（`r`）
- 保留了**历史梯度的指数衰减平均**（`s`），相当于**动量**。
Adam=Momentum（一阶）+RMSProp（二阶）
<a id="markdown-adam-算法描述" name="adam-算法描述"></a>
##### Adam 算法描述
![avatar](https://github.com/imhuay/Algorithm_Interview_Notes-Chinese/raw/master/_assets/TIM%E6%88%AA%E5%9B%BE20180611220109.png)
$$m_t=\gamma m_{t-1}+(1-\gamma)g_t\\
v_t=\rho v_{t-1}+(1-\rho)g_t^2\\
\hat{m_t}=\frac{m_t}{1-\gamma}\\
\hat{n_t}=\frac{n_t}{1-\rho}\\
\theta:=\theta-\frac{\eta}{\sqrt{\hat{v_t}}+\epsilon}\hat{m_t}$$


<a id="markdown-如何选择这些优化算法" name="如何选择这些优化算法"></a>
## 如何选择这些优化算法
- 各自适应学习率的优化算法表现不分伯仲，没有哪个算法能在所有任务上脱颖而出；
- 目前，最流行并且使用很高的优化算法包括 SGD、带动量的 SGD、RMSProp、带动量的 RMSProp、AdaDelta 和 Adam。

<a id="markdown-各优化算法的可视化" name="各优化算法的可视化"></a>
### 各优化算法的可视化
- SGD 各优化方法在损失曲面上的表现
![avatar](https://github.com/imhuay/Algorithm_Interview_Notes-Chinese/raw/master/_assets/contours_evaluation_optimizers.gif)

- SGD 各优化方法在**鞍点**处上的表现
![avatar](https://github.com/imhuay/Algorithm_Interview_Notes-Chinese/raw/master/_assets/saddle_point_evaluation_optimizers.gif)
