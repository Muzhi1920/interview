<a id="markdown-dl算法实现" name="dl算法实现"></a>
# DL算法实现
<!-- TOC -->

- [DL算法实现](#dl算法实现)
  - [一、反向传播](#一反向传播)
    - [计算损失](#计算损失)
      - [损失函数](#损失函数)
      - [隐层输出](#隐层输出)
      - [梯度下降](#梯度下降)
      - [激活函数](#激活函数)
      - [反向传播](#反向传播)
    - [输出层$i->j$](#输出层i-j)
    - [隐藏层$j->k$](#隐藏层j-k)
  - [卷积具体实现](#卷积具体实现)
  - [QA](#qa)
    - [1、矩阵运算比for循环快？](#1矩阵运算比for循环快)

<!-- /TOC -->

<a id="markdown-一反向传播" name="一反向传播"></a>
## 一、反向传播
![](https://upload-images.jianshu.io/upload_images/2256672-6f27ced45cf5c0d8.png)

---
<a id="markdown-计算损失" name="计算损失"></a>
### 计算损失
<a id="markdown-损失函数" name="损失函数"></a>
#### 损失函数
$$E_d\equiv\frac{1}{2}\sum_{i\in outputs}(t_i-y_i)^2$$
<a id="markdown-隐层输出" name="隐层输出"></a>
#### 隐层输出
$$net_{j}=\vec{w_j}·\vec{x_j}=\sum_{i}{w_{ji}}x_{ji}$$
<a id="markdown-梯度下降" name="梯度下降"></a>
#### 梯度下降
$$w_{ji}\gets w_{ji}-\eta\frac{\partial{E_d}}{\partial{w_{ji}}}$$
<a id="markdown-激活函数" name="激活函数"></a>
#### 激活函数
$$a_j=\sigma(net_j)$$
<a id="markdown-反向传播" name="反向传播"></a>
#### 反向传播
$$\frac{\partial{E_d}}{\partial{w_{ji}}}=\frac{\partial{E_d}}{\partial{net_j}}·\frac{\partial{net_j}}{\partial{w_{ji}}}\\
=\frac{\partial{E_d}}{\partial{net_j}}·\frac{\partial{\sum_{i}{w_{ji}}x_{ji}}}{\partial{w_{ji}}}\\
=\frac{\partial{E_d}}{\partial{net_j}}·x_{ji}$$
对于$\frac{\partial{E_d}}{\partial{net_j}}$的推导，需要区分输出层和隐藏层两种情况:

<a id="markdown-输出层i-j" name="输出层i-j"></a>
### 输出层$i->j$
$$\frac{\partial{E_d}}{\partial{net_j}}=\frac{\partial{E_d}}{\partial{y_j}}·\frac{\partial{y_j}}{\partial{net_j}}\\$$
$$\frac{\partial{E_d}}{\partial{y_j}}=\frac{\partial}{\partial{y_j}}\frac{1}{2}\sum_{i\in outputs}(t_i-y_i)^2
=\frac{\partial}{\partial{y_j}}\frac{1}{2}(t_j-y_j)^2
=-(t_j-y_j)$$
$$\frac{\partial{y_j}}{\partial{net_j}}=\frac{\partial sigmoid(net_j)}{\partial{net_j}}
=y_j(1-y_j)$$
故
$$\frac{\partial{E_d}}{\partial{net_j}}=-(t_j-y_j)y_j(1-y_j)$$
同时令
$$\delta_j=(t_j-y_j)y_j(1-y_j)$$


<a id="markdown-隐藏层j-k" name="隐藏层j-k"></a>
### 隐藏层$j->k$

$$\frac{\partial{E_d}}{\partial{net_j}}=\sum_{k\in Downstream(j)}\frac{\partial{E_d}}{\partial{net_k}}·\frac{\partial{net_k}}{\partial{net_j}}\\
=\sum_{k\in Downstream(j)}-\delta_k·\frac{\partial{net_k}}{\partial{net_j}}\\
=\sum_{k\in Downstream(j)}-\delta_k·\frac{\partial{net_k}}{\partial{a_j}}·\frac{\partial{a_j}}{\partial{net_j}}\\
=\sum_{k\in Downstream(j)}-\delta_k·w_{kj}·\frac{\partial{a_j}}{\partial{net_j}}\\
=\sum_{k\in Downstream(j)}-\delta_k·w_{kj}·a_j(1-a_j)\\
=-a_j(1-a_j)\sum_{k\in Downstream(j)}\delta_kw_{kj}$$

因为$\delta_j=-\frac{\partial{E_d}}{\partial{net_j}}$，带入上式得到：
$$\delta_j=a_j(1-a_j)\sum_{k\in Downstream(j)}\delta_kw_{kj}$$


<a id="markdown-卷积具体实现" name="卷积具体实现"></a>
## 卷积具体实现
![](https://pic2.zhimg.com/v2-42a950960eca8a61a4d4ec075ceed5b1_r.jpg)


<a id="markdown-qa" name="qa"></a>
## QA
<a id="markdown-1矩阵运算比for循环快" name="1矩阵运算比for循环快"></a>
### 1、矩阵运算比for循环快？
for循环访问内存，cache代价较大；矩阵分块顺序，并拷贝到另一片内存空间使其达到更好的访存效率；
  
- cache不变，矩阵越大，越难装进cache中，高速缓存缺失将缺失更多
- 矩阵大小不变，cache越大，越容易装更多的数据，高速缓存缺失将缺失更少

分块矩阵乘法的子块计算和正常的矩阵乘法是一样的，只不过在分块的时候传给函数的矩阵起始地址会有变化，需要变化为对应块的起始地址
