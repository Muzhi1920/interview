# interview
- 面试要点整理

## 一句话总结

### word2vec
kip-gram：基于负采样使用中心词预测上下文，不断调整中心词的词向量，实现序列向量化；
CBOW：使用上下文预测中心词，梯度同等作用到上下文中；

### Transformer
基于自注意力机制提取序列的特征，扩展到多头提取不同角度的信息，可以并行；

### GBDT XGBoost
GBDT基于boosting增强策略的加法模型，采用前向分步计算贪婪的学习，每次迭代都学习一棵CART树来拟合之前 t-1 棵树的预测结果与训练样本真实值的残差。
XGBoost是GBDT的优化和工程实现，比如牛顿法、正则项、支持并行和默认缺失值处理等，在可扩展性和训练速度上有了巨大的提升，但其核心思想没有大的变化。

### FM
实现特征二阶交叉，简化复杂度


- 场景业务
- 团队定位于目标
- 期望产出
- highcount

![](knowledge.png)