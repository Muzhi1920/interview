# 面试整理

## 第一部分：机器学习
### 1、层序与负采样，CBOW与skip-gram？
#### 1.1 层序与负采样
- 层序用哈夫曼树，词频高的路径短，更易被搜索，对于生僻词路径更长，搜索树深更深；多个LR二分类；
- 负采样：词频越大更被定为负样本，所以对生僻词更有优势
#### 1.2 CBOW与skip-gram
- CBOW： 训练次数等同于样本次数V(词典大小)；效率高，质量较差
- SKip-gram：训练次数为KV,即每个样本训练K次(窗口大小)；效率较低，质量高

### 2、如何选取特征？
特征选取包含一些特征处理的操作：
- 特征清洗（缺失，异常，样本比例权重）
- **标准化**和归一化）

标准化：更好保持样本间距；符合统计假设

什么时候需要标准化：**正则一定标准化**.$w$的大小与特征数值范围有关；此外标准化w大小可以反映不同特征对样本的贡献度。**与测试集分开标准化**

特征选取方法：树模型（组合，特征重要性）；DNN；业务相关统计特征（线性与非线性特征转换）

### 4、XGB细节
[XGBoost整理](https://github.com/NLP-LOVE/ML-NLP/tree/master/Machine%20Learning/3.3%20XGBoost#11-xgboost%E6%A0%91%E7%9A%84%E5%AE%9A%E4%B9%89)
#### 4.1 缺失值处理
xgboost把缺失值数据分到左子树和右子树分别计算损失，选择较优的那一个。如果训练中没有数据缺失，预测时出现了数据缺失，那么默认被分类到右子树

#### 4.2 树怎么生长
枚举所有不同树结构的贪心法，用打分函数来寻找出一个最优结构的树，每次在上一次的预测基础上取最优进一步分裂。
#### 4.3 树怎么停止生长或循环生成
- 分裂带来的增益小于设定阀值$\lambda$；
- 树达到最大深度时则停止建立决策树；
- 某个节点内样本权重和小于设定阈值时。

#### 4.4 如何实现并行
boosting技术中下一棵树依赖上述树的训练和预测，所以树与树之间应该是只能串行。**只是在选择最佳分裂点，进行枚举的时候并行，节点内选择最佳分裂点，候选分裂点计算增益用多线程并行。也是树形成最耗时的阶段**

#### 4.5 XGB与GBDT
- GBDT是算法，XGBoost工程实现；
- XGBoost显式地加入了正则项；
- GBDT使用代价函数的一阶导数信息，XGBoost可以同时使用一阶和二阶导数；
- GBDT采用CART，XGBoost支持多种类型的基分类器，比如线性分类器；
- GBDT在每轮迭代使用全部数据，XGBoost采用数据采样。
- 传统的GBDT没有设计对缺失值进行处理，XGBoost能够自动学习出缺失值的处理策略。



### 5、FM好处
1. 降低数据稀疏导致特征较差参数学习不充分的影响；
2. 提升参数学习效率和模型预估能力。


## 第二部分：深度学习（NLP）
### 1、LSTM与RNN，Transformer比较
RNN：易梯度消失；（梯度<1反向传播越乘越小或者越大）
LSTM：缓解梯度消失；缓解长期依赖；但路径复杂，模型训练复杂
Transformer：可并行计算，解决长期依赖

#### 1.1 为何出现梯度消失和梯度爆炸，LSTM又是如何解决（改善）的
原因：链式求导法则，导致梯度表示为连乘积的形式，梯度过大或者过小
忘记门：通过相乘加和形式保证梯度流传播稳定；
输入门：与普通RNN类似仍然是连乘形式，依旧会可能发生梯度消失；
通过改善一条路径的梯度缓解总体的远距离梯度太弱的问题。


### 2、Transformer：PE，残差和正则
#### 2.1 Trm的位置编码
原因：Trm打乱输入顺序依然会实现自注意力机制，仅计算两两之间的attention值，这样只会得到强大的词袋模型；因为是无序的；
解决：通过三角函数实现位置编码

![avatar](https://math.jianshu.com/math?formula=PE(pos%2C2i)%20%3D%20sin(%5Cfrac%7Bpos%7D%7B10000%5E%7B%5Cfrac%7B2i%7D%7Bd_%7Bmodel%7D%7D%7D%7D))
![avatar](https://math.jianshu.com/math?formula=PE(pos%2C2i%2B1)%20%3D%20cos(%5Cfrac%7Bpos%7D%7B10000%5E%7B%5Cfrac%7B2i%7D%7Bd_%7Bmodel%7D%7D%7D%7D))

三角函数的两条性质可以既考虑到绝对位置又可以考虑到相对位置。

#### 2.2 残差网络
原因：实际上随着网络深度的加深，训练错误会先减少，然后增多；深度越深意味着用优化算法越难训练，冗余的网络层学习了不是恒等映射的参数造成，DNN(x)!=x不是恒等映射后面就会学偏。
解决：引入残差网络希望这些冗余层能够完成恒等映射，保证DNN(x)=x。有助于解决梯度消失和梯度爆炸问题，让我们在训练更深网络的同时，又能保证良好的性能。




## 第三部分：推荐系统
- uctr和ppui含义



## 2、Wide&Depp
- Wide模型利用交叉特征高效的实现记忆能力，实现精准推荐
- Embedding类的模型通过学习到的低维稠密向量，实现模型的泛化推荐（未见过的内容）

- DeepFM

















