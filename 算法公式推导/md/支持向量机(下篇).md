## <center>支持向量机(下篇)</p>

<center>杨航锋</center>

#### 支持向量机的二次优化模型

​	在支持向量机(上篇)中推导出了对偶形式下的二次优化模型，为了不失一般性引入核函数，即求解下述问题
$$
\begin{aligned}
\mathop{\arg\min}_{\alpha}&\ \dfrac{1}{2}\sum\limits_{i=1,j=1}^m\alpha_i\alpha_jy^{(i)}y^{(j)}k(x^{(i)},x^{(j)})-\sum\limits_{i=1}^m\alpha_i\\
&s.t.
\begin{cases} 
\sum\limits_{i=1}^m\alpha_iy^{(i)}=0\\
\alpha_i\geqslant0,i=1,2,\cdots,m
\end{cases}
\end{aligned}
$$
想知道上式详细推导过程的读者可以去[ML-NOTE](https://github.com/yhangf/ML-NOTE)下载，当然你也可以在我的[知乎专栏](https://zhuanlan.zhihu.com/jiqixuexi)进行阅读。

#### SMO算法求解模型

##### SMO算法的基本思想

​	在SMO算法迭代过程中，每一次需要选择一对变量$\ \{(\alpha_i,\alpha_j)|1\leqslant i\leqslant m,1\leqslant j\leqslant m,i\ne j\}\ $其他$\ m-2\ $个变量看成常数项，因为在支持向量机模型的约束条件中需要满足$\ \sum\limits_{i=1}^m\alpha_iy^{(i)}=0\ $，因此如果改变其中的一个$\ \alpha_i\ $那么至少需要改变一个$\ \alpha_j\ $才能满足约束条件，故SMO算法将一个复杂的二次优化问题转化为求解若干个比较简单的二变量优化问题。 为了方便推导不妨选择$\ \alpha_1、\alpha_2\ $作为初次迭代对象，记$\ k_{i,j}=k(x^{(i)},x^{(j)}),T=\sum\limits_{i=3}^m\alpha_i\ $则支持向量机的二次优化模型可以改写成
$$
\begin{aligned}
\mathop{\arg\min}_{\alpha_1,\alpha_2}&\ \dfrac{1}{2}k_{1,1}\alpha_1^2+\dfrac{1}{2}k_{2,2}\alpha_2^2+y^{(1)}y^{(2)}k_{1,2}\alpha_1\alpha_2-(\alpha_1+\alpha_2)+y^{(1)}\alpha_1\sum\limits_{i=3}^m\alpha_iy^{(i)}k_{i,1}\\
&+y^{(2)}\alpha_2\sum\limits_{i=3}^my^{(i)}\alpha_ik_{i,2}-T
\\
&s.t.
\begin{cases} 
\alpha_1y^{(1)}+\alpha_2y^{(2)}=-\sum\limits_{i=3}^m\alpha_iy^{(i)}=\gamma \\
\alpha_i\geqslant0,i=1,2,\cdots,m
\end{cases}
\end{aligned}
$$
##### SMO算法求解过程

![](F:\ML-NOTE\picture\smo.png)

​	如图所示$\ \alpha_1、\alpha_2\ $之间的关系被限制在正方形框中的线段中，所以二变量的优化问题实际上仅仅是一个变量的优化问题。 记上一轮迭代得到的解为$\ \alpha_1^{old}、\alpha_2^{old}\ $，沿着约束方向$\ \alpha_2\ $没有经过约束的解为$\ \alpha_2^{new,unr}\ $，该轮迭代结束后的解为$\ \alpha_1^{new}、\alpha_2^{new}\ $。不妨假设$\ L、H\ $分别为$\ \alpha_2^{new}\ $的下界和上界，因此有
$$
L\leqslant\alpha_2^{new}\leqslant H
$$
成立，那么怎么求解$\ L、H\ ​$呢？有已知条件当$\ y^{(1)}\ne y^{(2)}\ ​$时有(根据上图分析)
$$
\begin{aligned}
&\begin{cases}
\alpha_1^{new}-\alpha_2^{new}=\gamma\\
\alpha_1^{old}-\alpha_2^{old}=\gamma
\end{cases}
\Rightarrow
\alpha_1^{new}=\alpha_2^{new}+(\alpha_1^{old}-\alpha_2^{old})\\
\\
&\begin{cases}
0\leqslant\alpha_1^{new}\leqslant C\\
0\leqslant\alpha_2^{new}\leqslant C
\end{cases}
\Rightarrow
\begin{cases}
0\leqslant\alpha_2^{new}+(\alpha_1^{old}-\alpha_2^{old})\leqslant C\\
0\leqslant\alpha_2^{new}\leqslant C
\end{cases}\\ \\
&\Rightarrow

\begin{cases}
\alpha_2^{old}-\alpha_1^{old}\leqslant\alpha_2^{new}\leqslant C+(\alpha_2^{old}-\alpha_1^{old})\\
0\leqslant\alpha_2^{new}\leqslant C
\end{cases}
\end{aligned}
$$
所以
$$
\begin{aligned}
\begin{cases}
L=max\{0,\alpha_2^{old}-\alpha_1^{old}\}\\
H=min\{C,C+\alpha_2^{old}-\alpha_1^{old}\}
\end{cases}
\end{aligned}
$$
同理可得，当$\ y^{(1)}=y^{(2)}\ $时有
$$
\begin{aligned}
\begin{cases}
L=max\{0,\alpha_2^{old}+\alpha_1^{old}-C\}\\
H=min\{C,\alpha_2^{old}+\alpha_1^{old}\}
\end{cases}
\end{aligned}
$$
也就是说当对二元函数$\ \phi(\alpha_1,\alpha_2)\ $求导得到无约束情况下的解$\ \alpha_2^{new,unr}\ $，则最终$\ \alpha_2^{new}\ $应该为
$$
\begin{aligned}
\alpha_2^{new}
=
\begin{cases}
H, & \alpha_2^{new,unr}\gt H\\
\alpha_2^{new,unr}, & L\leqslant\alpha_2^{new,unr}\leqslant H\\
L,& \alpha_2^{new,unr}<L
\end{cases}
\end{aligned}
$$
现在问题就转化为怎么求解$\ \alpha_2^{new,unr}\ ​$了。

​	对$\ \alpha_1y^{(1)}+\alpha_2y^{(2)}=-\sum\limits_{i=3}^m\alpha_iy^{(i)}=\gamma\ $两边同乘以$\ y^{(1)}\ $可得$\ \alpha_1(y^{(1)})^2+\alpha_2y^{(1)}y^{(2)}=\gamma y^{(1)}\ $又因为$\ (y^{(1)})^2=1\ $故$\ \alpha_1=(\gamma-\alpha_2y^{(2)})y^{(1)}\ $，然后将$\ \alpha_1\ $回带到原二次优化函数中消去$\ \alpha_1 \ $可得
$$
\begin{aligned}
\mathop{\arg\min}_{\alpha_2}\ &\phi(\alpha_2)=\ \dfrac{1}{2}k_{1,1}(\gamma-\alpha_2y^{(2)})^2+\dfrac{1}{2}k_{2,2}\alpha_2^2+y^{(2)}k_{1,2}(\gamma-\alpha_2y^{(2)})\alpha_2-((\gamma-\alpha_2y^{(2)})y^{(1)}+\alpha_2)\\
&+(\gamma-\alpha_2y^{(2)})\sum\limits_{i=3}^m\alpha_iy^{(i)}k_{i,1}
+y^{(2)}\alpha_2\sum\limits_{i=3}^my^{(i)}\alpha_ik_{i,2}-T

\end{aligned}
$$
记$\ u=\sum\limits_{i=3}^m\alpha_iy^{(i)}k_{i,1}、v=\sum\limits_{i=3}^my^{(i)}\alpha_ik_{i,2}\ $,常数$\ T\  $不影响$\ \phi(\alpha_2)\ $的驻点因而省略，则
$$
\begin{aligned}
\mathop{\arg\min}_{\alpha_2}\ &\phi(\alpha_2)=\ \dfrac{1}{2}k_{1,1}(\gamma-\alpha_2y^{(2)})^2+\dfrac{1}{2}k_{2,2}\alpha_2^2+y^{(2)}k_{1,2}(\gamma-\alpha_2y^{(2)})\alpha_2-((\gamma-\alpha_2y^{(2)})y^{(1)}+\alpha_2)\\
&+(\gamma-\alpha_2y^{(2)})u
+y^{(2)}\alpha_2v

\end{aligned}
$$
至此，$\ \phi(\alpha_2)\ $就是一个一元函数了，要求驻点只需令其导数为$\ 0\ $即可
$$
\begin{equation}
\begin{split}
\phi '(\alpha_2)=
&-k_{1,1}y^{(2)}(\gamma-\alpha_2y^{(2)})+k_{2,2}\alpha_2+k_{1,2}y^{(2)}\gamma-2k_{1,2}\alpha_2\\
&+y^{(1)}y^{(2)}-1-uy^{(2)}+vy^{(2)}\\

=&(k_{1,1}-2k_{1,2}+k_{2,2})\alpha_2-k_{1,1}y^{(2)}\gamma+k_{1,2}y^{(2)}\gamma+y^{(1)}y^{(2)}\\
&-1-uy^{(2)}+vy^{(2)}
\\
=&0
\end{split}

\end{equation}
$$
在求解上式之前需要做一些准备工作：

1. 优化前的解$\ \alpha_1^{old}、\alpha_2^{old}\ $满足下面等式约束条件
   $$
   \alpha_1^{old}y^{(1)}+\alpha_2^{old}y^{(2)}=\gamma
   $$

2. 假设支持向量机的超平面方程为$\ f(x)=w^Tx+b \ $，带入$\ w\ $的表达式则$\ f(x)=\sum\limits_{i=1}^m\alpha_iy^{(i)}k(x^{(i)},x)+b \ $，$\ f(x^{(i)})\ $表示样本$\ x^{(i)}\ $的预测值，$\ y^{(i)} \ $表示样本$\ x^{(i)} \ $的真实值，定义$\ E^{(i)} \ $为预测值和真实值之差，即
   $$
   E^{(i)}=f(x^{(i)})-y^{(i)}
   $$

3. 由于$\ u=\sum\limits_{i=3}^m\alpha_iy^{(i)}k_{i,1}、v=\sum\limits_{i=3}^my^{(i)}\alpha_ik_{i,2}\ $，所以
   $$
   \begin{aligned}
   u&=f(x^{(1)})-\sum\limits_{j=1}^2\alpha_jy^{(j)}k_{j,1}-b\\
   v&=f(x^{(2)})-\sum\limits_{j=1}^2\alpha_jy^{(j)}k_{j,2}-b
   \end{aligned}
   $$


下面接着求$\ \phi(\alpha_2)\ $的驻点，并带入准备工作中的式子化简可得
$$
\begin{equation}
\begin{split}
(k_{1,1}-2k_{1,2}+k_{2,2})\alpha_2^{new,unr}&=y^{(2)}(y^{(2)}-y^{(1)}+\gamma k_{1,1}-\gamma k_{1,2}+u-v)\\
&=y^{(2)}\left[y^{(2)}-y^{(1)}+\gamma k_{1,1}-\gamma k_{1,2}+(f(x^{(1)})-\sum\limits_{j=1}^2y^{(j)}\alpha_jk_{1,j}-b)\\
(f(x^{(2)})-\sum\limits_{j=1}^2y^{(j)}\alpha_jk_{1,j}-b)
\right]\\
&=y^{(2)}\left[y^{(2)}(k_{1,1}-2k_{1,2}+k_{2,2})\alpha_2^{old}+(f(x^{(1)}-y^{(1)})-(f(x^{(2)})-y^{(2)})
\right]\\
&=(k_{1,1}-2k_{1,2}+k_{2,2})\alpha_2^{old}+y^{(2)}(E^{(1)}-E^{(2)})
\end{split}
\end{equation}
$$
最后可以得到$\ \alpha_2^{new,unr} \ $的表达式为
$$
\alpha_2^{new,unr}=\alpha_2^{old}+\dfrac{y^{(2)}(E^{(1)}-E^{(2)})}{k_{1,1}-2k_{1,2}+k_{2,2}}
$$
因此根据$\ \alpha_2^{new,unr} \ ​$和$\ \alpha_2^{new} \ ​$的关系式就可以迭代更新$\ \alpha_2^{new} \ ​$，然后再利用$\ \alpha_1^{new} \ ​$和$\ \alpha_2^{new} \ ​$的线性关系$\ \alpha_1^{old}y^{(1)}+\alpha_2^{old}y^{(2)}=\alpha_1^{new}y^{(1)}+\alpha_2^{new}y^{(2)} \ ​$就可以更新$\ \alpha_1^{new} \ ​$。

​	在每次更新完两个$\ \alpha_i、\alpha_j\ $之后，需要重新计算出阈值$\ b\ $因为$\ b\ $的值关系到$\ f(x)\ $和下次迭代时误差$\ E^{(i)}\ $的计算。简单回顾一下支持向量机中的KKT条件
$$
\begin{aligned}
\begin{cases}
\alpha_i=0\Longleftrightarrow y^{(i)}(w^Tx^{(i)}+b)\geqslant1,\\
\alpha_i=C\Longleftrightarrow y^{(i)}(w^Tx^{(i)}+b)\leqslant1,\\
0\lt \alpha_i\lt C\Longleftrightarrow y^{(i)}(w^Tx^{(i)}+b)=1,
\end{cases}
\end{aligned}
$$
为了使得被优化的样本都满足KKT条件， 当$\ 0\lt \alpha_1^{new}\lt C\ ​$时则相应的数据点为支持向量且有下式成立
$$
y^{(1)}(w^Tx^{(1)}+b)=1\Rightarrow
\sum\limits_{i=1}^m\alpha_iy^{(i)}k_{i,1}+b=y^{(1)}
$$
于是新的$\  b_1^{new} \ $为
$$
b_1^{new}=y^{(1)}-\sum\limits_{i=3}^m\alpha_iy^{(i)}k_{i,1}-\alpha_1^{new}y^{(1)}k_{1,1}-\alpha_2^{new}y^{(2)}k_{2,1}
$$
根据$\ E^{(1)}\ $的定义可知
$$
E^{(1)}=\sum\limits_{i=3}^m\alpha_iy^{(i)}k_{i,1}+\alpha_1^{old}y^{(1)}k_{1,1}+\alpha_2^{old}y^{(2)}k_{2,1}+b^{old}-y^{(1)}\\
y^{(1)}-\sum\limits_{i=3}^m\alpha_iy^{(i)}k_{i,1}=-E^{(1)}+\alpha_1^{old}y^{(1)}k_{1,1}+\alpha_2^{old}y^{(2)}k_{2,1}+b^{old}
$$
回带到$\ b_1^{new} \ $得到
$$
b_1^{new}=-E^{(1)}-y^{(1)}k_{1,1}(\alpha_1^{new}-\alpha_1^{old})-y^{(2)}k_{2,1}(\alpha_2^{new}-\alpha_2^{old})+b^{old}
$$
同理可得，若$\ 0\lt \alpha_2^{new}\lt C\ $那么
$$
b_2^{new}=-E^{(2)}-y^{(1)}k_{1,2}(\alpha_1^{new}-\alpha_1^{old})-y^{(2)}k_{2,2}(\alpha_2^{new}-\alpha_2^{old})+b^{old}
$$
如果$\ \alpha_1^{new}、\alpha_2^{new} \ $同时满足条件$\ 0\lt \alpha_i^{new}\lt C, i=1,2\ $，那么有$\ b_1^{new}=b_2^{new} \ $；如果$\ \alpha_1^{new}、\alpha_2^{new} \ $是$\ 0\ $或者是$\ C\ $，那么$\ b_1^{new}、b_2^{new}\ $以及它们之间的数都是满足KKT条件的阈值，这时取$\ b^{new}=\dfrac{b_1^{new}+b_2^{new}}{2} \ $。最后还需要更新对应的$\ E_i^{new}\ $,其中$\ G\ $代表的所有支持向量$\ x^{(j)}\ $的集合
$$
E_i^{new}=\sum\limits_{G}\alpha_jy^{(j)}k_{i,j}+b^{new}-y^{(i)}
$$
以上就是SMO算法的完整流程。

#### SMO算法总结

​	假设输入空间是$\ X\in\R^{n} \ $,$\ Y\in\{+1,-1\}\ $，不妨假设含有$m$个样本数据($x^{(1)}$,$y^{(1)}$)、($x^{(2)}$,$y^{(2)}$)、$\cdots$、($x^{(m)}$,$y^{(m)}$)，其中$x^{(i)}\in X、y^{(i)}\in Y \ $，精度为$\ \varepsilon\ $，输出近似解为$\ \widehat{\alpha} \ $；

1. 取初值$\ \alpha^0=0\ $，令$\ k=0\ $

2. 选取待优化变量$\ \alpha_1^k、\alpha_2^k\ $，计算出新的$\ \alpha_2^{new,unr}\ $
   $$
   \alpha_2^{new,unr}=\alpha_2^{old}+\dfrac{y^{(2)}(E^{(1)}-E^{(2)})}{k_{1,1}-2k_{1,2}+k_{2,2}}
   $$

3. 更新$\ \alpha_2^{k+1}\ $
   $$
   \begin{aligned}
   \alpha_2^{k+1}
   =
   \begin{cases}
   H, & \alpha_2^{new,unr}\gt H\\
   \alpha_2^{new,unr}, & L\leqslant\alpha_2^{new,unr}\leqslant H\\
   L,& \alpha_2^{new,unr}<L
   \end{cases}
   \end{aligned}
   $$

4. 利用$\ \alpha_1^{k+1}\ $和$\ \alpha_2^{k+1}\ $的关系求出$\ \alpha_1^{k+1}\ $
   $$
   \alpha_1^{k+1}=\alpha_1^{k}+y^{(1)}y^{(2)}(\alpha_2^{k}-\alpha_2^{k+1})
   $$

5. 计算$\ b^{k+1}\ $和$\ E_i\ $

6. 在精度$\ \varepsilon\ $范围内检查是否满足如下的终止条件，其中$\ f(x^{(i)})=\sum\limits_{j=1}^m\alpha_jy^{(j)}k_{j,i}+b \ $ 
   $$
   \begin{aligned}
   &\begin{cases}
   \sum\limits_{i=1}^m\alpha_iy^{(i)}=0,\\
   0\leqslant\alpha_i\leqslant C,i=1,2,\cdots,m
   \end{cases}\\
   \\
   &\begin{cases}
   \alpha_i^{k+1}=0\Longleftrightarrow y^{(i)}f(x^{(i)})\geqslant1,\\
   \alpha_i^{k+1}=C\Longleftrightarrow y^{(i)}f(x^{(i)})\leqslant1,\\
   0\lt \alpha_i^{k+1}\lt C\Longleftrightarrow y^{(i)}f(x^{(i)})=1,
   
   \end{cases}
   \end{aligned}
   $$
   若满足则转$\ 7\ $；否则令$\ k=k+1 \ $转到步骤$\ 2\ $；

7. 取$\ \widehat{\alpha}=\alpha^{k+1} \ $。

   

   