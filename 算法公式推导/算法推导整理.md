<!-- TOC -->

- [公式推导整理](#公式推导整理)
  - [EM算法](#em算法)
    - [求解含有隐变量的概率模型](#求解含有隐变量的概率模型)
    - [收敛性证明](#收敛性证明)
    - [结论](#结论)
  - [朴素贝叶斯](#朴素贝叶斯)
      - [朴素贝叶斯模型的求解](#朴素贝叶斯模型的求解)
  - [GBDT vs XGBDT](#gbdt-vs-xgbdt)
    - [目标函数](#目标函数)
      - [决策树函数$f_k$](#决策树函数f_k)
      - [决策树的复杂度](#决策树的复杂度)
      - [XGBoost的目标函数](#xgboost的目标函数)
    - [决策树构造](#决策树构造)
  - [$FM算法$](#fm算法)
    - [$FM$算法小结](#fm算法小结)
  - [L1，L2正则](#l1l2正则)
    - [从贝叶斯先验概率看正则化](#从贝叶斯先验概率看正则化)
    - [$L1$正则化的概率解释](#l1正则化的概率解释)
    - [$L2$正则化的概率解释](#l2正则化的概率解释)
  - [LSTM与GRU](#lstm与gru)
  - [PCA降维](#pca降维)
  - [感知机模型](#感知机模型)
  - [BP反向传播算法](#bp反向传播算法)
    - [函数与输出](#函数与输出)
    - [输出层$i->j$](#输出层i-j)
    - [隐藏层$j->k$](#隐藏层j-k)
  - [回归算法与最小二乘](#回归算法与最小二乘)
    - [正规方程](#正规方程)
    - [总结](#总结)
  - [K-means](#k-means)
    - [k-means算法的损失函数](#k-means算法的损失函数)
    - [优化损失函数](#优化损失函数)

<!-- /TOC -->

<a id="markdown-公式推导整理" name="公式推导整理"></a>
# 公式推导整理

<a id="markdown-em算法" name="em算法"></a>
## EM算法
当已知每一个样本数据$x^{(i)}$都对应一个类别变量$z^{(i)}$时，即$z=(z^{(1)},z^{(2)},\dots,z^{(m)})$，此时的极大化模型的对数似然函数可以通过全概率公式展开为
$$
\begin{aligned}
\hat{\theta}&= \mathop{\arg\max}_{\theta} \sum\limits_{i=1}^m\log p(x^{(i)};\theta)\\
&= \mathop{\arg\max}_{\theta} \sum\limits_{i=1}^m\log \sum\limits_{z^{(i)}}p(x^{(i)},z^{(i)};\theta)
\end{aligned}
$$
因为含有隐变量$z$故极大似然估计并不能够求解上述模型。
<a id="markdown-求解含有隐变量的概率模型" name="求解含有隐变量的概率模型"></a>
### 求解含有隐变量的概率模型
通过引入隐变量$z^{(i)}$的概率分布为$Q_i(z^{(i)})$，因为$\log (x)$是凹函数故结合凹函数形式下的詹森不等式进行放缩处理
$$
\begin{aligned}
\sum\limits_{i=1}^m\log \sum\limits_{z^{(i)}}p(x^{(i)},z^{(i)};\theta)&=\sum\limits_{i=1}^m\log \sum\limits_{z^{(i)}} Q_i(z^{(i)})\dfrac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}\\
&=\sum\limits_{i=1}^m\log \mathbb{E}(\dfrac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})})\\
&\ge\sum\limits_{i=1}^m\mathbb{E}[\log\dfrac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})})]\\
&=\sum\limits_{i=1}^m\sum\limits_{z^{(i)}}Q_i(z^{(i)})\log\dfrac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}
\end{aligned}
$$
当不等式变成等式时说明调整后的概率能够等价于$\mathcal{L}(\theta)$，所以必须找到使得等式成立的条件，即寻找
$$
\mathbb{E}[\log\dfrac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})})]=\log \mathbb{E}[\dfrac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}]
$$
由期望得性质可知当
$$
\dfrac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}=C,\ \ \ \ \ C\in\R    \ \ \ \ \ (*)
$$
等式成立，对上述等式进行变形处理可得
$$
\begin{aligned}
&p(x^{(i)},z^{(i)};\theta)=CQ_i(z^{(i)})\\
&\Leftrightarrow
\sum\limits_{z^{(i)}}p(x^{(i)},z^{(i)};\theta)=C\sum\limits_{z^{(i)}}Q_i(z^{(i)})=C\\
&\Leftrightarrow
\sum\limits_{z^{(i)}}p(x^{(i)},z^{(i)};\theta)=C \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \   (**)
\end{aligned}
$$
把$\ (**)\ ​$式带入$\ (*)\ ​$化简可知
$$
\begin{aligned}
Q_i(z^{(i)})&=\dfrac{p(x^{(i)},z^{(i)};\theta)}{\sum\limits_{z^{(i)}}p(x^{(i)},z^{(i)};\theta)}\\
&=\dfrac{p(x^{(i)},z^{(i)};\theta)}{p(x^{(i)};\theta)}\\
&=p(z^{(i)}|x^{(i)};\theta)
\end{aligned}
$$
至此$Q_i(z^{(i)})$的计算公式就是后验概率，解决了$Q_i(z^{(i)})$如何选择得问题。这一步称为$E$步，建立 $\mathcal{L}(\theta)$得下界；接下来得$M$步，就是在给定$Q_i(z^{(i)})$后，调整$\theta$去极大化$\mathcal{L}(\theta)$的下界即
$$
\begin{aligned}
&\mathop{\arg\max}_{\theta}\sum\limits_{i=1}^m\log p(x^{(i)};\theta)\\
&\Leftrightarrow
\mathop{\arg\max}_{\theta}\sum\limits_{i=1}^m\sum\limits_{z^{(i)}}Q_i(z^{(i)})\log\dfrac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}\\
&\Leftrightarrow
\mathop{\arg\max}_{\theta}\sum\limits_{i=1}^m\sum\limits_{z^{(i)}}Q_i(z^{(i)})\left[\log p(x^{(i)},z^{(i)};\theta)-\log Q_i(z^{(i)})\right]\\
&\Leftrightarrow
\mathop{\arg\max}_{\theta}\sum\limits_{i=1}^m\sum\limits_{z^{(i)}}Q_i(z^{(i)})\log p(x^{(i)},z^{(i)};\theta)
\end{aligned}
$$
因此EM算法的迭代形式为\
Repeats until it converges{\
$E$ step：for every  $x^{(i)}$ calculate\
$Q_i(z^{(i)})=p(z^{(i)}|x^{(i)};\theta)$\
$M$ step：update  $\theta$
$\begin{aligned} \theta:=\mathop{\arg\max}_{\theta}\sum\limits_{i=1}^m\sum\limits_{z^{(i)}}Q_i(z^{(i)})\log p(x^{(i)},z^{(i)};\theta) \end{aligned}$\
}
<a id="markdown-收敛性证明" name="收敛性证明"></a>
### 收敛性证明
$$
\begin{aligned}
\mathcal{L}(\theta^{(k)})&=\sum\limits_{i=1}^m\sum\limits_{z^{(i)}}Q_i^{(k)}(z^{(i)})\log \dfrac{p(x^{(i)},z^{(i)};\theta^{(k)})}{Q_i(z^{(i)})} \ \ \ \ \ \ \ \ \ \ (a)\\
&\le \sum\limits_{i=1}^m\sum\limits_{z^{(i)}}Q_i^{(k)}(z^{(i)})\log \dfrac{p(x^{(i)},z^{(i)};\theta^{(k)})}{Q_i(z^{(i)})}\ \ \ \ \ \ \ \ \ \ (b)\\
&\le\mathcal{L}(\theta^{(k+1)})\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \  (c)
\end{aligned}
$$
首先$(a)$式是前面$E$步所保证詹森不等式中的等式成立的条件，$(a)$到$(b)$是M步的定义，$(b)$到$(c)$对任意参数都成立，而其等式的条件是固定$\theta$并调整好$Q$时成立，$(b)$到$(c)$只是固定$Q$调整$\theta$，在得到$\theta^{(k+1)}$时，只是最大化$\mathcal{L}(\theta^{(k)})$，也就是$\mathcal{L}(\theta^{(k+1)})$的一个下界而没有使等式成立。
<a id="markdown-结论" name="结论"></a>
### 结论
EM算法可以保证收敛到一个稳定点，但是却不能保证收敛到全局的极大值点，因此它是局部最优的算法，当然，如果我们的优化目标L(θ,θj)是凸的，则EM算法可以保证收敛到全局最大值，这点和梯度下降法这样的迭代算法相同。


<a id="markdown-朴素贝叶斯" name="朴素贝叶斯"></a>
## 朴素贝叶斯
$$P(Y=c_k|X=x^{(i)}) = \dfrac{P(X=x^{(i)}|Y=c_k)P(Y=c_k)}{\sum\limits_{k=1}^K P(X=x^{(i)}|Y=c_k)P(Y=c_k)} \ \ \ \ \ \ \ \ \ \ \ (*)$$
由于朴素贝叶斯算法对条件概率分布作了条件独立性假设，故
$$
\begin{aligned}
P(X=x^{(i)}|Y=c_k)&=P(X_1^{(i)}=x_1^{(i)},\cdots,X_n^{(i)}=x_n^{(i)}|Y=c_k)\\
&=\prod_{j=1}^n P(X_j^{(i)}=x_j^{(i)}|Y=c_k)
\end{aligned} \ \ \ \ \    (**)
$$
将$(**)$式带入$(*)$式中得到朴素贝叶斯算法的基本形式：
$$
\begin{aligned}
P(Y=c_k|X=x^{(i)}) =\dfrac{\prod\limits_{j=1}^n P(X_j^{(i)}=x_j^{(i)}|Y=c_k)}{\sum\limits_{k=1}^K P(Y=c_k)\prod\limits_{j=1}^n P(X_j^{(i)}=x_j^{(i)}|Y=c_k)}P(Y=c_k)
\end{aligned}
$$
因此朴素贝叶斯算法的优化模型为
$$
\begin{aligned}
\mathop{\arg\max}_{c_k}\ P(Y=c_k|X=x^{(i)})&=\mathop{\arg\max}_{c_k}\ \dfrac{\prod\limits_{j=1}^n P(X_j^{(i)}=x_j^{(i)}|Y=c_k)}{\sum\limits_{k=1}^t P(Y=c_k)\prod\limits_{j=1}^n P(X_j^{(i)}=x_j^{(i)}|Y=c_k)}P(Y=c_k)\\
&=\mathop{\arg\max}_{c_k}\ P(Y=c_k)\prod\limits_{j=1}^n P(X_j^{(i)}=x_j^{(i)}|Y=c_k)
\end{aligned}
$$
因为对于每一个类别$c_k$，分母$\sum\limits_{k=1}^t P(Y=c_k)\prod\limits_{j=1}^n P(X_j^{(i)}=x_j^{(i)}|Y=c_k)$的值都是相同的。

<a id="markdown-朴素贝叶斯模型的求解" name="朴素贝叶斯模型的求解"></a>
#### 朴素贝叶斯模型的求解

​求解朴素贝叶斯模型相当于求解几个概率值，对于样本数据集可以求出先验概率

- $p(Y=c_k)$
$$
p(Y=c_k)=\dfrac{\sum\limits_{i=1}^mI(y^{(i)}=c_k)}{t}\ \ \ \ \ k=1,2,\cdots,t
$$
其中$I(x)$表示当$x$为真时函数值为$1$否则为$0$。
- 条件概率
$$
\begin{aligned}
P(X_j^{(i)}=x_j^{(i)}|Y&=c_k)=\dfrac{\sum\limits_{i=1}^mI(x_j^{(i)}=a_{jl},y^{(i)}=c_k)}{\sum\limits_{i=1}^m I(y^{(i)}=c_k)}\\
&j=1,2,\cdots,n\\
&l=1,2,\cdots,S_j
\end{aligned}
$$
其中$x_j^{(i)}$代表第$i$个样本的第$j$个特征$x_j^{(i)}\in\{a_{j1},a_{j2},\cdots,a_{jS_j}\}$，$a_{jl}$表示第$j$个特征取得第$l$个值。

<a id="markdown-gbdt-vs-xgbdt" name="gbdt-vs-xgbdt"></a>
## GBDT vs XGBDT
GBDT目标函数最终形式为： 
$$ Obj^{(k)}=\sum\limits_{i=1}^m L(y^{(i)},\hat{y}^{(i)}_{k-1})+g_if_k(x^{(i)})$$

XGBDT目标函数最终形式为： 
$$ \begin{aligned} Obj^{(k)}&==\sum\limits_{i=1}^mL(y^{(i)},\hat{y}^{(i)}_{k-1}+f_k(x^{(i)}))+\Omega(f_k)+C \end{aligned} $$

---
<a id="markdown-目标函数" name="目标函数"></a>
### 目标函数

为了求损失函数$L(y^{(i)},\hat{y}^{(i)}_{k-1}+f_k(x^{(i)}))​$在$\ \hat{y}^{(i)}_{k-1} \ ​$处的二阶展开，且记$\nabla_{\hat{y}^{(i)}_{k-1}}L\left(y^{(i)},\hat{y}^{(i)}_{k-1}\right)$为$g_i$、$\nabla^2_{\hat{y}^{(i)}_{k-1}}L\left(y^{(i)},\hat{y}^{(i)}_{k-1}\right)$为$h_i​$则有：
$$
L\left(y^{(i)},\hat{y}^{(i)}_{k-1}+f_k(x^{(i)})\right)\simeq L(y^{(i)},\hat{y}^{(i)}_{k-1})+g_if_k(x^{(i)})+\dfrac{1}{2}h_if^2_k(x^{(i)})$$
又因为在第$k$步$\hat{y}^{(i)}_{k-1}$其实是已知的，所以$l(y^{(i)},\hat{y}^{(i)}_{k-1})$是一个常数函数，故对优化目标函数不会产生影响，将上述结论带入目标函数$Obj^{(k)}$可得：
$$Obj^{(k)}\simeq\sum\limits_{i=1}^m\bigg[ g_if_k(x^{(i)})+\dfrac{1}{2}h_if^2_k(x^{(i)})\bigg]+\Omega(f_k)$$

<a id="markdown-决策树函数f_k" name="决策树函数f_k"></a>
#### 决策树函数$f_k$

对于任意决策树$f_k$，假设其叶子结点个数$T​$，该决策树是由所有结点对应的值组成的向量$\ w\in\mathbb{R}^T\ ​$，以及能够把特征向量映射到叶子结点的函数$\ q(*):\mathbb{R}^d\rightarrow \{1,2,\cdots,T \} \ ​$构造而成的，且每个样本数据都存在唯一的叶子结点上。因此决策树$\ f_k\ ​$可以定义为$\ f_k(x)=w_{q(x)} \ ​$。

<a id="markdown-决策树的复杂度" name="决策树的复杂度"></a>
#### 决策树的复杂度

由正则项$\ \Omega(f_k)=\gamma T+\dfrac{1}{2}\lambda\sum\limits_{j=1}^Tw_j^2 \ ​$来定义，该正则项表明决策树模型的复杂度可以由叶子结点的数量和叶子结点对应值向量$\ w \ ​$的$\ L2\ ​$范数决定。定义集合$\ I_j=\{i|q(x^{(i)})=j \}\ ​$为划分到叶子结点$\ j \ ​$的所有训练样本的集合，即之前训练样本的集合，现在都改写成叶子结点的集合。

<a id="markdown-xgboost的目标函数" name="xgboost的目标函数"></a>
#### XGBoost的目标函数
因此$\ XGBoost\ ​$算法的目标函数可以改写为：
$$\begin{aligned}
Obj^{(k)}&\simeq\sum\limits_{i=1}^m\bigg[ g_if_k(x^{(i)})+\dfrac{1}{2}h_if^2_k(x^{(i)})\bigg]+\Omega(f_k)\\
&=\sum\limits_{i=1}^m\bigg[g_iw_{q(x^{(i)})}+\dfrac{1}{2}h_jw^2_{q(x^{(i)})} \bigg]+\gamma T+\dfrac{1}{2}\lambda\sum\limits_{j=1}^Tw_j^2\\
&=\sum\limits_{j=1}^T\bigg[(\sum\limits_{i\in I_j}g_i)w_j+\dfrac{1}{2}(\sum\limits_{i\in I_j}h_i+\lambda)w_j^2 \bigg]+\gamma T
\end{aligned}$$
令$G_j=\sum\limits_{i\in I_j}g_i ,\ H_j=\sum\limits_{i\in I_j}h_i$则有：
$$Obj^{(k)}\simeq\sum\limits_{j=1}^T\bigg[G_jw_j+\dfrac{1}{2}(H_j+\lambda)w_j^2 \bigg]$$

分析可知当更新到第$\ k\ ​$步时，此时**决策树结构固定的情况下**，每个叶子结点有哪些样本是已知的，那么$\ q(*)\ ​$和$\ I_j\ ​$也是已知的；又因为$\ g_i\ ​$和$\ h_i\ ​$是第$\ k-1\ ​$步的导数，那么也是已知的，因此$\ G_j\ ​$和$\ H_j\ ​$都是已知的。令目标函数$\ Obj^{(k)}\ ​$的一阶导数为$\ 0\ ​$，即可求得叶子结点$\ j\ ​$对应的值为：
$$w^*_j=-\dfrac{G_j}{H_j+\lambda}$$

因此针对于结构固定的决策树，最优的目标函数$Obj$为：
$$Obj=-\dfrac{1}{2}\sum\limits_{j=1}^T\dfrac{G_j^2}{H_j+\lambda}+\gamma T$$

<a id="markdown-决策树构造" name="决策树构造"></a>
### 决策树构造
通常使用贪心策略来生成决策树的每个结点，$XGBoost$算法的在决策树的生成阶段就对过拟合的问题进行了处理，因此无需独立的剪枝阶段，具体步骤可以归纳为：
1. 从深度为$0$的树开始对每个叶子结点穷举所有的可用特征；
2. 针对每一个特征，把属于该结点的训练样本的该特征升序排列，通过线性扫描的方式来决定该特征的**最佳分裂点**，并采用最佳分裂点时的**收益**；
3. 选择收益最大的特征作为分裂特征，用该特征的最佳分裂点作为分裂位置，把该结点生成出左右两个新的叶子结点，并为每个新结点关联新的样本集；
4. 退回到第一步，继续递归操作直到满足特定条件。

因为对某个结点采取的是二分策略，分别对应左子结点和右子结点，除了当前待处理的结点，其他结点对应的$\ Obj \ ​$值都不变，所以对于收益的计算只需要考虑当前结点的$\ Obj \ ​$值即可，分裂前针对该结点的最优目标函数为：
$$Obj^{(before)}=-\dfrac{1}{2}\dfrac{(G_L+G_R)^2}{(H_L+H_R)+\lambda}+\gamma$$
分裂后的最优目标函数为：
$$Obj^{(later)}=-\dfrac{1}{2}\bigg[\dfrac{G_L^2}{H_L+\lambda}+\dfrac{G_R^2}{H_R+\lambda} \bigg]+2\gamma$$
那么对于该目标函数来说，分裂后的收益为：
$$\begin{aligned}
Gain&=Obj^{(before)}-Obj^{(later)}\\
&=\dfrac{1}{2}\bigg[\dfrac{G_L^2}{H_L+\lambda}+\dfrac{G_R^2}{H_R+\lambda}-\dfrac{(G_L+G_R)^2}{(H_L+H_R)+\lambda} \bigg]-\gamma
\end{aligned}$$
故可以用上述公式来决定最有分裂特征和最优特征分裂点。

<a id="markdown-fm算法" name="fm算法"></a>
## $FM算法$
$$LR：
y=w_0+\sum\limits_{i=1}^nw_ix_i
$$
**在数据非常稀疏的情况下很难满足$x_i、x_j$都不为$0$，这样将会导致$w_{ij}$不能够通过训练得到**，引入辅助向量$V$用$w_{ij}=\mathbf{v}_i\mathbf{v}_j^T$对$w$进行分解
$$
w=VV^T=
\begin{bmatrix}
\mathbf{v}_1\\
\mathbf{v}_2\\
\vdots\\
\mathbf{v}_n
\end{bmatrix}
\begin{bmatrix}
\mathbf{v}_1^T &
\mathbf{v}_2^T & 
\cdots & 
\mathbf{v}_n^T
\end{bmatrix}
$$
综上可以发现原始模型的二项式参数为$\frac{n(n-1)}{2}$个，现在减少为$kn(k\ll n)$个。引入辅助向量$V$最为重要的一点是使得$x_tx_i$和$x_ix_j$的参数不再相互独立，这样就能够在样本数据稀疏的情况下合理的估计模型交叉项的参数
$$
\begin{aligned}
\langle\mathbf{v}_t,\mathbf{v}_i\rangle&=\sum\limits_{f=1}^k\mathbf{v}_{tf}\cdot\mathbf{v}_{if}\\
\langle\mathbf{v}_i,\mathbf{v}_j\rangle&=\sum\limits_{f=1}^k\mathbf{v}_{if}\cdot\mathbf{v}_{jf}
\end{aligned}
$$
**$x_tx_i$和$x_ix_j$的参数分别为$\langle\mathbf{v}_{t},\mathbf{v}_i \rangle$和$\langle\mathbf{v}_{i},\mathbf{v}_j \rangle$，它们之间拥有共同项$\mathbf{v}_i$，即所有包含$\mathbf{v}_i$的非零组合特征的样本都可以用来学习隐向量$\mathbf{v}_i$，而原始模型中$w_{ti}$和$w_{ij}$却是相互独立的，这在很大程度上避免了数据稀疏造成的参数估计不准确的影响。因此原始模型可以改写为最终的$FM$算法**
$$
y=w_0+\sum\limits_{i=1}^nw_ix_i+\sum\limits_{i=1}^{n-1}\sum\limits_{j=i+1}^n \langle\mathbf{v}_i,\mathbf{v}_j \rangle x_ix_j
$$
由于求解上述式子的时间复杂度为$\mathcal{O}(n^2)$，可以看出主要是最后一项计算比较复杂，因此从数学上对该式最后一项进行一些改写可以把时间复杂度降为$\mathcal{O}(kn)$
$$
\begin{aligned} & \sum_{i=1}^{n-1} \sum_{j=1}^{n}\left\langle\mathbf{v}_{i}, \mathbf{v}_{j}\right\rangle x_{i} x_{j} \\=& \frac{1}{2} \sum_{i=1}^{n} \sum_{j=1}^{n}\left\langle\mathbf{v}_{i}, \mathbf{v}_{j}\right\rangle x_{i} x_{j}-\frac{1}{2} \sum_{i=1}^{n}\left\langle\mathbf{v}_{i}, \mathbf{v}_{i}\right\rangle x_{i} x_{i} \\=& \frac{1}{2}\left(\sum_{i=1}^{n} \sum_{j=1}^{n} \sum_{f=1}^{k} \mathbf{v}_{if} \mathbf{v}_{jf} x_{i} x_{j}-\sum_{i=1}^{n} \sum_{f=1}^{k} \mathbf{v}_{if} \mathbf{v}_{if} x_{i} x_{i}\right) \\=& \frac{1}{2} \sum_{f=1}^{k}\left(\left(\sum_{i=1}^{n} \mathbf{v}_{if} x_{i}\right)\left(\sum_{j=1}^{n} \mathbf{v}_{jf} x_{j}\right)-\sum_{i=1}^{n} \mathbf{v}_{if}^{2} x_{i}^{2}\right) \\=& \frac{1}{2} \sum_{f=1}^{k}\left(\left(\sum_{i=1}^{n} \mathbf{v}_{if} x_{i}\right)^{2}-\sum_{i=1}^{n} \mathbf{v}_{if}^{2} x_{i}^{2}\right) \end{aligned}
$$

<a id="markdown-fm算法小结" name="fm算法小结"></a>
### $FM$算法小结

- $FM$算法降低了因数据稀疏，导致特征交叉项参数学习不充分的影响；
- $FM$算法提升了参数学习效率和模型预估的能力。


<a id="markdown-l1l2正则" name="l1l2正则"></a>
## L1，L2正则
L1正则加上L1范数；L2正则加上L2范数
$L1$正则化和$L2$正则化的符号化描述假设待优化函数为$f(\theta)$，其中$\theta\in\R^n$，那么优化问题可以转化为求
$$\mathop{\arg\min}_{\theta}\ f(\theta)$$

$L1$正则化，即对参数$\theta$加上$L1$范数约束
$$\mathop{\arg\min}_{\theta}\ J_1(\theta)=f(\theta)+\color{magenta}\lambda\Vert\theta\Vert_1$$

$L2$正则化，即对参数$\theta$加上$L2$范数的平方约束
$$\mathop{\arg\min}_{\theta}\ J_2(\theta)=f(\theta)+\color{magenta}\lambda\Vert\theta\Vert_2^2 $$


<a id="markdown-从贝叶斯先验概率看正则化" name="从贝叶斯先验概率看正则化"></a>
### 从贝叶斯先验概率看正则化
假设输入空间是$X\in\R^{n}$,输出空间是$Y$，不妨假设含有$m$个样本数据($x^{(1)}$,$y^{(1)}$)、($x^{(2)}$,$y^{(2)}$)、$\cdots$、($x^{(m)}$,$y^{(m)}$)，其中$x^{(i)}\in X、y^{(i)}\in Y$。贝叶斯学派认为参数$\theta$也是服从某种概率分布的，即先给定$\theta$的先验分布为$p(\theta)$，然后根据贝叶斯定理$\color{Red}P(\theta|(X, Y))= \dfrac{P((Y,X);\theta)\times P(\theta)}{P(X,Y)}\sim P(Y|X;\theta)\times P(\theta)$（这里的$Y|X$仅仅是一种记号，代表给定的$X$对应相关的$Y$），因此通过极大似然估计可求参数$\theta$。
$$
\mathop{\arg\max}_{\theta}\ L(\theta)=\prod_{i=1}^{m}p(y^{(i)}|x^{(i)};\theta)p(\theta)
$$
等价于求解对数化极大似然函数$L(\theta)$
$$
\begin{aligned}
\mathop{\arg\max}_{\theta}\ L(\theta)&=\log L(\theta)\\
&=\sum\limits_{i=1}^m\log p(y^{(i)}|x^{(i)};\theta)+\sum\limits_{i=1}^m\log p(\theta)\\ \\
\Leftrightarrow \mathop{\arg\min}_{\theta}\ -L(\theta)&=-\log L(\theta) \\
&=-\sum\limits_{i=1}^m\log p(y^{(i)}|x^{(i)};\theta)-\sum\limits_{i=1}^m\log p(\theta)\\
&=f(\theta)\color{magenta}-\sum\limits_{i=1}^m\log p(\theta)
\end{aligned}
$$

<a id="markdown-l1正则化的概率解释" name="l1正则化的概率解释"></a>
### $L1$正则化的概率解释
假设$\theta$服从的先验分布为均值为$0$参数为$\lambda$的拉普拉斯分布，即$\theta\sim La(0, \lambda)$其中，$p(\theta)= \frac{1}{2\lambda}e^{- \frac{|\theta|}{\lambda}}$。因此，上述优化函数可转换为：
$$\begin{aligned}
&\mathop{\arg\min}_{\theta}\ f(\theta)\color{magenta}-\sum\limits_{i=1}^m\log p(\theta)\\
&=f(\theta)-\sum\limits_{i=1}^m\log \frac{1}{2\lambda}e^{-\frac{|\theta_i|}{\lambda}}\\
&=f(\theta)-\sum\limits_{i=1}^m \log\frac{1}{2\lambda} + \frac{1}{\lambda}\sum\limits_{i=1}^m|\theta_i|\\
&\Leftrightarrow \mathop{\arg\min}_{\theta}\ f(\theta) + \color{magenta}\lambda\Vert\theta\Vert_1
\end{aligned}$$

**从上面的数学推导可以看出，$L1$正则化可以看成是：通过假设权重参数$\theta​$的先验分布为拉普拉斯分布，由最大后验概率估计导出。**​	


<a id="markdown-l2正则化的概率解释" name="l2正则化的概率解释"></a>
### $L2$正则化的概率解释
假设$\theta$服从的先验分布为均值为$0$方差为$\sigma^2$的正态分布，即$\theta\sim \mathcal{N} (0, \sigma^2)$其中，$p(\theta)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{\theta^2}{2\sigma^2}}$。因此，上述优化函数可转换为：
$$\begin{aligned}
&\mathop{\arg\min}_{\theta}\ f(\theta)\color{magenta}-\sum\limits_{i=1}^m\log p(\theta)\\
&=f(\theta)-\sum\limits_{i=1}^m\log \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{\theta_i^2}{2\sigma^2}}\\
&=f(\theta)-\sum\limits_{i=1}^m \log \frac{1}{\sqrt{2\pi}\sigma} + \frac{1}{2\sigma^2}\sum\limits_{i=1}^m \theta_i^2\\
&\Leftrightarrow\mathop{\arg\min}_{\theta} f(\theta) + \color{magenta}\lambda\Vert\theta\Vert_2^2
\end{aligned}
$$
**从上面的数学推导可以看出，$L2$正则化可以看成是：通过假设权重参数$\theta$的先验分布为正态分布，由最大后验概率估计导出。**


<a id="markdown-lstm与gru" name="lstm与gru"></a>
## LSTM与GRU
LSTM：忘记门+输入门+输出门
GRU：重置门+更新门

<a id="markdown-pca降维" name="pca降维"></a>
## PCA降维
最大方差原理：
1. 对m\*n数据按列组成n*m；
2. 对数据，按行进行0均值化；
3. 求协方差矩阵
$$C=\dfrac{1}{m}XX^T\ $$
4. 对C求特征值，特征向量；按大小排列，取前K行
5. Y=P*X 为X降维后的K维数据

<a id="markdown-感知机模型" name="感知机模型"></a>
## 感知机模型

$$\begin{aligned} sign(x) = 
\begin{cases}
+1, & ifx \geqslant 0  \\
-1, &{otherwise}
\end{cases}
\end{aligned}$$
因此可知感知机的假设函数为$\psi (x)$,通过输入特征向量$x$即可判断其所属类别。

误分类点集$M$中所有点到超平面$S$的距离之和为
$$ -\dfrac{1}{\Vert \omega\Vert_2}\sum\limits_{(x^{(i)},y^{(i)})\in M}y^{(i)}(w^Tx^{(i)}+b)$$
所以求解最小化距离之和可知
$$\begin{aligned} \mathop{\arg\min}_{\omega, b} L(\omega,b) & \Leftrightarrow \mathop{\arg\min}_{\omega, b} -\dfrac{1}{\Vert \omega\Vert_2}\sum\limits_{(x^{(i)},y^{(i)})\in M}y^{(i)}(w^Tx^{(i)}+b)\ & \Leftrightarrow \mathop{\arg\min}_{\omega, b} -\sum\limits_{(x^{(i)},y^{(i)})\in M}y^{(i)}(w^Tx^{(i)}+b)\ \end{aligned}$$ 
综上可知，感知机算法的损失函数为
$$L(\omega,b)=-\sum\limits_{(x^{(i)},y^{(i)})\in M}y^{(i)}(w^Tx^{(i)}+b)$$


<a id="markdown-bp反向传播算法" name="bp反向传播算法"></a>
## BP反向传播算法
<a id="markdown-函数与输出" name="函数与输出"></a>
### 函数与输出
$$E_d\equiv\frac{1}{2}\sum_{i\in outputs}(t_i-y_i)^2$$
- 隐层输出
$$net_{j}=\vec{w_j}·\vec{x_j}=\sum_{i}{w_{ji}}x_{ji}$$
- 梯度下降
$$w_{ji}\gets w_{ji}-\eta\frac{\partial{E_d}}{\partial{w_{ji}}}$$
- 激活函数
$$a_j=\sigma(net_j)$$
- 反向传播
$$\frac{\partial{E_d}}{\partial{w_{ji}}}=\frac{\partial{E_d}}{\partial{net_j}}·\frac{\partial{net_j}}{\partial{w_{ji}}}\\
=\frac{\partial{E_d}}{\partial{net_j}}·\frac{\partial{\sum_{i}{w_{ji}}x_{ji}}}{\partial{w_{ji}}}\\
=\frac{\partial{E_d}}{\partial{net_j}}·x_{ji}$$
对于$\frac{\partial{E_d}}{\partial{net_j}}$的推导，需要区分输出层和隐藏层两种情况:

<a id="markdown-输出层i-j" name="输出层i-j"></a>
### 输出层$i->j$
$$\frac{\partial{E_d}}{\partial{net_j}}=\frac{\partial{E_d}}{\partial{y_j}}·\frac{\partial{y_j}}{\partial{net_j}}\\$$
$$\frac{\partial{E_d}}{\partial{y_j}}=\frac{\partial}{\partial{y_j}}\frac{1}{2}\sum_{i\in outputs}(t_i-y_i)^2
=\frac{\partial}{\partial{y_j}}\frac{1}{2}(t_j-y_j)^2
=-(t_j-y_j)$$
$$\frac{\partial{y_j}}{\partial{net_j}}=\frac{\partial sigmoid(net_j)}{\partial{net_j}}
=y_j(1-y_j)$$
故
$$\frac{\partial{E_d}}{\partial{net_j}}=-(t_j-y_j)y_j(1-y_j)$$
同时令
$$\delta_j=(t_j-y_j)y_j(1-y_j)$$


<a id="markdown-隐藏层j-k" name="隐藏层j-k"></a>
### 隐藏层$j->k$

$$\frac{\partial{E_d}}{\partial{net_j}}=\sum_{k\in Downstream(j)}\frac{\partial{E_d}}{\partial{net_k}}·\frac{\partial{net_k}}{\partial{net_j}}\\
=\sum_{k\in Downstream(j)}-\delta_k·\frac{\partial{net_k}}{\partial{net_j}}\\
=\sum_{k\in Downstream(j)}-\delta_k·\frac{\partial{net_k}}{\partial{a_j}}·\frac{\partial{a_j}}{\partial{net_j}}\\
=\sum_{k\in Downstream(j)}-\delta_k·w_{kj}·\frac{\partial{a_j}}{\partial{net_j}}\\
=\sum_{k\in Downstream(j)}-\delta_k·w_{kj}·a_j(1-a_j)\\
=-a_j(1-a_j)\sum_{k\in Downstream(j)}\delta_kw_{kj}$$

因为$\delta_j=-\frac{\partial{E_d}}{\partial{net_j}}$，带入上式得到：
$$\delta_j=a_j(1-a_j)\sum_{k\in Downstream(j)}\delta_kw_{kj}$$



<a id="markdown-回归算法与最小二乘" name="回归算法与最小二乘"></a>
## 回归算法与最小二乘
对于二次函数梯度下降算法最终会收敛到全局最小值。下面使用梯度下降算法求解上面推导出的线性回归模型的损失函数$\ J(\theta)=\dfrac{1}{2}\sum\limits_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2$，为了实现该算法首先要求出损失函数$J(\theta)$对参数$\theta_j$的梯度：
$$
\begin{aligned}
\nabla_{\theta_j} J(\theta) &=\nabla_{\theta_j} \dfrac{1}{2} \sum\limits_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2\\
&=2\cdot\dfrac{1}{2}\sum\limits_{i=1}^m(h_\theta(x^{(i)})-y^{(i)}) \nabla_{\theta_j}(h_\theta(x^{(i)})-y^{(i)})\\
&=\sum\limits_{i=1}^m(h_\theta(x^{(i)})-y^{(i)}) \nabla_{\theta_j}\theta^Tx^{(i)}\\
&=\sum\limits_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}
\end{aligned}
$$
因此在线性回归模型中利用所有的样本数据，训练梯度下降算法的完整迭代格式为
$$
\theta_j:=\theta_j-\alpha \sum\limits_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)} \ \ \ ( j\ \  for \ \ 0 \ \sim \ n)
$$
上述迭代过程每次迭代都会使用所有的样本数据，数学上已经证明线性回归模型的损失函数通过梯度下降算法求解一定会全局收敛，所以如果要编程实现该算法只需要控制迭代次数即可，不过对于线性回归模型的求解一般不用梯度下降算法，还有更容易实现且更快捷的形式—正规方程。

<a id="markdown-正规方程" name="正规方程"></a>
### 正规方程

​对损失函数$J(\theta)=\dfrac{1}{2}\sum\limits_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2$改写成矩阵乘法的形式，在此之前需要先定义一些矩阵，不妨令：
$$
Y=\begin{bmatrix}
(y^{(1)}) \\ (y^{(2)}) \\ \vdots \\ (y^{(m)})
\end{bmatrix}
~\\
X=\begin{bmatrix}
(x^{(1)})^T \\ (x^{(2)})^T \\ \vdots \\ (x^{(m)})^T
\end{bmatrix}
=\begin{bmatrix}
1 & x_1^{(1)} & x_2^{(1)} & \cdots & x_n^{(1)} \\
1 & x_1^{(2)} & x_2^{(2)} & \cdots & x_n^{(2)} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
1 & x_1^{(m)} & x_2^{(m)} & \cdots & x_n^{(m)}
\end{bmatrix}$$


由于$h_\theta(x^{(i)})=\theta^Tx^{(i)}=(x^{(i)})^T\theta$，因此可以得出
$$
X\theta  - Y= 
\begin{bmatrix}
(x^{(1)})^T\theta \\
(x^{(2)})^T\theta \\
\vdots\\
(x^{(m)})^T\theta
\end{bmatrix}

-
\begin{bmatrix}
(y^{(1)}) \\ (y^{(2)}) \\ \vdots \\ (y^{(m)})
\end{bmatrix}

=
\begin{bmatrix}
(x^{(1)})^T\theta - (y^{(1)}) \\ (x^{(2)})^T\theta -  (y^{(2)}) \\ \vdots \\ (x^{(m)})^T\theta -  (y^{(m)})
\end{bmatrix}
$$
然后根据$\sum\limits_{i=1}^n\phi_i^2=\phi^T\phi$,综上可以得出$J(\theta)$的矩阵形式
$$
J(\theta)=\dfrac{1}{2} \sum\limits_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2=\dfrac{1}{2}(X\theta - Y)^T(X\theta-Y)
$$
最后求解$\begin{aligned}\mathop{\arg\min}_{\theta} \dfrac{1}{2}(X\theta-Y)^T(X\theta-Y) \end{aligned}$即可。

​在求解上述优化问题之前先简单的介绍下矩阵求导法则
$$
\begin{aligned}
\nabla_x x^Tb
&=
    \nabla_x
    \begin{bmatrix}
    x_1, & x_2, & \cdots, & x_n
    \end{bmatrix}
    \begin{bmatrix}
    b_1\\ b_2\\ \vdots \\ b_n
    \end{bmatrix}\\
&=\nabla_{x_i} \sum\limits_{i=1}^nx_ib_i\\
&=b\\

\nabla_x Ax &= 
\nabla_{x}
\begin{bmatrix}
    a_{11}  & a_{12} & \cdots  & a_{1n}\\
     a_{21}  & a_{22} & \cdots  & a_{2n}\\
     \vdots & \vdots & \ddots & \vdots\\
      a_{n1}  & a_{n2} & \cdots  & a_{nn}
 \end{bmatrix}
\begin{bmatrix}
    x_1 \\ x_2\\ \vdots \\ x_n
 \end{bmatrix}\\
&=
\begin{bmatrix}
\nabla_{x_i}\sum\limits_{i=1}^na_{1i}x_i \\
\nabla_{x_i}\sum\limits_{i=1}^na_{2i}x_i \\
\vdots\\
\nabla_{x_i}\sum\limits_{i=1}^na_{ni}x_i
\end{bmatrix}\\
&=
A^T

\end{aligned}
$$
所以
$$
\begin{aligned}
\nabla_\theta J(\theta) &= \nabla_\theta \ \dfrac{1}{2}(X\theta-Y)^T(X\theta-Y) \\
&=\dfrac{1}{2} \nabla_\theta \ (\theta^TX^T-Y^T)(X\theta-Y) \\
&=\dfrac{1}{2} \nabla_\theta \ (\theta^TX^TX\theta-\theta^TX^TY-Y^TX\theta+Y^TY)\\
&=\dfrac{1}{2}(2X^TX\theta-2X^TY) \\
&=X^TX\theta-X^TY
\end{aligned}
$$
称$X^TX\theta=X^TY$为正规方程，因此$\theta=(X^TX)^{-1}X^TY$，实际上$X^TX$不可逆的情况非常少就算$X^TX$真的是不可逆也无妨，可以先对原始数据进行特征筛选或者正则化即可。

<a id="markdown-总结" name="总结"></a>
### 总结
一般情况下对于求解线性回归模型通常采用正规方程的方式，回过头来对于文章开头的求解一元线性回归方程系数的问题可以大大化简，具体参数形式为
$$\begin{bmatrix}
\alpha\\
\beta
 \end{bmatrix}
 =\left(\begin{bmatrix}
  1 & x_1^T\\
  1 & x_2^T\\
  \vdots & \vdots\\
  1 & x_n^T
 \end{bmatrix}^T
  \begin{bmatrix}
  1 & x_1^T\\
  1 & x_2^T\\
  \vdots & \vdots\\
  1 & x_n^T
 \end{bmatrix}\right)^{-1}
  \begin{bmatrix}
  1 & x_1^T\\
  1 & x_2^T\\
  \vdots & \vdots\\
  1 & x_n^T
 \end{bmatrix}^T
  \begin{bmatrix}
 y_1\\y_2\\ \vdots \\ y_n
 \end{bmatrix}$$
可以看出通过正规方程求解线性回归模型就转化为如何构造$X$矩阵，只要$X$矩阵构造出来后剩下的就是交给计算机做矩阵乘法运算就可以了。


<a id="markdown-k-means" name="k-means"></a>
## K-means
<a id="markdown-k-means算法的损失函数" name="k-means算法的损失函数"></a>
### k-means算法的损失函数

假设输入空间 $X \in \R^n$ 为$n$维向量的集合，$X=\{x^{(1)} ,x^{(2)},\cdots,x^{(m)} \}$，$\mathcal  C$为输入空间$X$的一个划分，不妨令$\mathcal C=\{ \mathbb C_1,\mathbb C_2,\cdots,\mathbb C_K \}$，因此可以定义$k-means$算法的损失函数为
$$
J(\mathcal C)=\sum\limits_{k=1}^K\sum\limits_{x^{(i)}\in \mathbb C_k}\Vert x^{(i)}-\mu^{(k)} \Vert_2^2
$$
其中$\mu^{(k)}=\frac{1}{\vert \mathbb C_k \vert}\sum\limits_{x^{(i)}\in\mathbb C_k}x^{(i)}$是簇$\mathbb C_k$的聚类中心。

<a id="markdown-优化损失函数" name="优化损失函数"></a>
### 优化损失函数

$k\text{-}means$算法的损失函数$J(\mathcal C)$描述了簇类样本围绕簇聚类中心的紧密程度，其值越小，则簇内样本的相似度越高。故$k\text{-}means$算法的优化目标为最小化损失函数
$$
\mathop{\argmin}_{c}\ J(\mathcal C)=\sum\limits_{k=1}^K\sum\limits_{x^{(i)}\in \mathbb C_k}\Vert x^{(i)}-\mu^{(k)} \Vert_2^2
$$
如果要优化该损失函数就需要考虑输入空间$\mathcal X$的所有划分，这是一个$NP\text{-}hard$问题，实际上是采取贪心的策略通过迭代优化来近似求解，该过程等价于[$EM$算法](https://zhuanlan.zhihu.com/p/39490840)。

1. 首先随机初始化$K$个聚类中心，$\mu^{(1)},\mu^{(2)},\cdots,\mu^{(K)}$；
2. 然后根据这$K$个聚类中心给出输入空间$\mathcal X$的一个划分，$\mathbb C_1,\mathbb C_2,\cdots,\mathbb C_K$；
   - 样本离哪个簇的聚类中心最近，则该样本就划归到那个簇
    $$\mathop{\arg\min}_{k}\ \Vert x^{(i)}-\mu^{(k)} \Vert_2^2$$
3. 再根据这个划分来更新这$K$个聚类中心
    $$
    \mu^{(k)}=\frac{1}{\vert \mathbb C_k \vert}\sum\limits_{x^{(i)}\in\mathbb C_k}x^{(i)}
    $$
4. 重复2、3步骤直至收敛
    - 即$K$个聚类中心不再变化
