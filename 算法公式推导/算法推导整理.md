<!-- TOC -->

- [公式推导整理](#公式推导整理)
  - [EM算法](#em算法)
    - [结论](#结论)
  - [二、GBDT vs XGBDT](#二gbdt-vs-xgbdt)
    - [1、目标函数](#1目标函数)
  - [三、L1,L2正则](#三l1l2正则)
  - [四、LSTM与GRU](#四lstm与gru)
  - [五、PCA降维](#五pca降维)
  - [六、感知机模型](#六感知机模型)

<!-- /TOC -->

<a id="markdown-公式推导整理" name="公式推导整理"></a>
# 公式推导整理

<a id="markdown-em算法" name="em算法"></a>
## EM算法

![EM收敛证明](https://images0.cnblogs.com/blog/381513/201310/29230243-01a2ee08ab7f4d7d93f93e36837f95be.png)

<a id="markdown-结论" name="结论"></a>
### 结论
EM算法可以保证收敛到一个稳定点，但是却不能保证收敛到全局的极大值点，因此它是局部最优的算法，当然，如果我们的优化目标L(θ,θj)是凸的，则EM算法可以保证收敛到全局最大值，这点和梯度下降法这样的迭代算法相同。

<a id="markdown-二gbdt-vs-xgbdt" name="二gbdt-vs-xgbdt"></a>
## 二、GBDT vs XGBDT
<a id="markdown-1目标函数" name="1目标函数"></a>
### 1、目标函数
GBDT目标函数最终形式为： 
$$ Obj^{(k)}=\sum\limits_{i=1}^m \left(l(y^{(i)},\hat{y}^{(i)}_{k-1})+g_if_k(x^{(i)})\right) $$

XGBDT目标函数最终形式为： 
$$ \begin{aligned} Obj^{(k)}&=\sum\limits_{i=1}^ml(y^{(i)},\hat{y}^{(i)}k)+\sum\limits_{i=1}^T{\Omega(f_i)}\ &=\sum\limits_{i=1}^ml(y^{(i)},\hat{y}^{(i)}_{k-1}+f_k(x^{(i)}))+\Omega(f_k)+C \end{aligned} $$

在决策树结构固定的情况下最优的目标函数:$Obj$:
$$ Obj=-\dfrac{1}{2}\sum\limits_{j=1}^T\dfrac{G_j^2}{H_j+\lambda}+\gamma T$$
然而决策树结构数量是无穷的，所以实际上并不能穷举所有可能的决策树结构，什么样的决策树结构是最优的呢？通常使用贪心策略来生成决策树的每个结点.

<a id="markdown-三l1l2正则" name="三l1l2正则"></a>
## 三、L1,L2正则
L1正则加上L1范数；L2正则加上L2范数
$L1$正则化和$L2$正则化的符号化描述假设待优化函数为$f(\theta)$，其中$\theta\in\R^n$，那么优化问题可以转化为求
$$\mathop{\arg\min}_{\theta}\ f(\theta)$$

$L1$正则化，即对参数$\theta$加上$L1$范数约束
$$\mathop{\arg\min}_{\theta}\ J_1(\theta)=f(\theta)+\color{magenta}\lambda\Vert\theta\Vert_1$$

$L2$正则化，即对参数$\theta$加上$L2$范数的平方约束
$$\mathop{\arg\min}_{\theta}\ J_2(\theta)=f(\theta)+\color{magenta}\lambda\Vert\theta\Vert_2^2 $$

<a id="markdown-四lstm与gru" name="四lstm与gru"></a>
## 四、LSTM与GRU
LSTM：忘记门+输入门+输出门
GRU：重置门+更新门

<a id="markdown-五pca降维" name="五pca降维"></a>
## 五、PCA降维
最大方差原理：
1. 对m\*n数据按列组成n*m；
2. 对数据，按行进行0均值化；
3. 求协方差矩阵
$$C=\dfrac{1}{m}XX^T\ $$
4. 对C求特征值，特征向量；按大小排列，取前K行
5. Y=P*X 为X降维后的K维数据

<a id="markdown-六感知机模型" name="六感知机模型"></a>
## 六、感知机模型

$$\begin{aligned} sign(x) = 
\begin{cases}
+1, & ifx \geqslant 0  \\
-1, &{otherwise}
\end{cases}
\end{aligned}$$
因此可知感知机的假设函数为$\psi (x)$,通过输入特征向量$x$即可判断其所属类别。

误分类点集$M$中所有点到超平面$S$的距离之和为
$$ -\dfrac{1}{\Vert \omega\Vert_2}\sum\limits_{(x^{(i)},y^{(i)})\in M}y^{(i)}(w^Tx^{(i)}+b)$$
所以求解最小化距离之和可知
$$\begin{aligned} \mathop{\arg\min}_{\omega, b} L(\omega,b) & \Leftrightarrow \mathop{\arg\min}_{\omega, b} -\dfrac{1}{\Vert \omega\Vert_2}\sum\limits_{(x^{(i)},y^{(i)})\in M}y^{(i)}(w^Tx^{(i)}+b)\ & \Leftrightarrow \mathop{\arg\min}_{\omega, b} -\sum\limits_{(x^{(i)},y^{(i)})\in M}y^{(i)}(w^Tx^{(i)}+b)\ \end{aligned}$$ 
综上可知，感知机算法的损失函数为
$$L(\omega,b)=-\sum\limits_{(x^{(i)},y^{(i)})\in M}y^{(i)}(w^Tx^{(i)}+b)$$

















