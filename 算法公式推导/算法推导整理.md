<!-- TOC -->

- [公式推导整理](#公式推导整理)
  - [EM算法](#em算法)
    - [求解含有隐变量的概率模型](#求解含有隐变量的概率模型)
    - [收敛性证明](#收敛性证明)
    - [结论](#结论)
  - [GBDT vs XGBDT](#gbdt-vs-xgbdt)
    - [1、目标函数](#1目标函数)
  - [三、L1,L2正则](#三l1l2正则)
  - [四、LSTM与GRU](#四lstm与gru)
  - [五、PCA降维](#五pca降维)
  - [六、感知机模型](#六感知机模型)
  - [BP反向传播算法](#bp反向传播算法)
      - [损失函数](#损失函数)
      - [隐层输出](#隐层输出)
      - [梯度下降](#梯度下降)
      - [激活函数](#激活函数)
      - [反向传播](#反向传播)
    - [输出层$i->j$](#输出层i-j)
    - [隐藏层$j->k$](#隐藏层j-k)

<!-- /TOC -->

<a id="markdown-公式推导整理" name="公式推导整理"></a>
# 公式推导整理

<a id="markdown-em算法" name="em算法"></a>
## EM算法
当已知每一个样本数据$x^{(i)}$都对应一个类别变量$z^{(i)}$时，即$z=(z^{(1)},z^{(2)},\dots,z^{(m)})$，此时的极大化模型的对数似然函数可以通过全概率公式展开为
$$
\begin{aligned}
\hat{\theta}&= \mathop{\arg\max}_{\theta} \sum\limits_{i=1}^m\log p(x^{(i)};\theta)\\
&= \mathop{\arg\max}_{\theta} \sum\limits_{i=1}^m\log \sum\limits_{z^{(i)}}p(x^{(i)},z^{(i)};\theta)
\end{aligned}
$$
因为含有隐变量$z$故极大似然估计并不能够求解上述模型。
<a id="markdown-求解含有隐变量的概率模型" name="求解含有隐变量的概率模型"></a>
### 求解含有隐变量的概率模型
通过引入隐变量$z^{(i)}$的概率分布为$Q_i(z^{(i)})$，因为$\log (x)$是凹函数故结合凹函数形式下的詹森不等式进行放缩处理
$$
\begin{aligned}
\sum\limits_{i=1}^m\log \sum\limits_{z^{(i)}}p(x^{(i)},z^{(i)};\theta)&=\sum\limits_{i=1}^m\log \sum\limits_{z^{(i)}} Q_i(z^{(i)})\dfrac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}\\
&=\sum\limits_{i=1}^m\log \mathbb{E}(\dfrac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})})\\
&\ge\sum\limits_{i=1}^m\mathbb{E}[\log\dfrac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})})]\\
&=\sum\limits_{i=1}^m\sum\limits_{z^{(i)}}Q_i(z^{(i)})\log\dfrac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}
\end{aligned}
$$
当不等式变成等式时说明调整后的概率能够等价于$\mathcal{L}(\theta)$，所以必须找到使得等式成立的条件，即寻找
$$
\mathbb{E}[\log\dfrac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})})]=\log \mathbb{E}[\dfrac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}]
$$
由期望得性质可知当
$$
\dfrac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}=C,\ \ \ \ \ C\in\R    \ \ \ \ \ (*)
$$
等式成立，对上述等式进行变形处理可得
$$
\begin{aligned}
&p(x^{(i)},z^{(i)};\theta)=CQ_i(z^{(i)})\\
&\Leftrightarrow
\sum\limits_{z^{(i)}}p(x^{(i)},z^{(i)};\theta)=C\sum\limits_{z^{(i)}}Q_i(z^{(i)})=C\\
&\Leftrightarrow
\sum\limits_{z^{(i)}}p(x^{(i)},z^{(i)};\theta)=C \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \   (**)
\end{aligned}
$$
把$\ (**)\ ​$式带入$\ (*)\ ​$化简可知
$$
\begin{aligned}
Q_i(z^{(i)})&=\dfrac{p(x^{(i)},z^{(i)};\theta)}{\sum\limits_{z^{(i)}}p(x^{(i)},z^{(i)};\theta)}\\
&=\dfrac{p(x^{(i)},z^{(i)};\theta)}{p(x^{(i)};\theta)}\\
&=p(z^{(i)}|x^{(i)};\theta)
\end{aligned}
$$
至此$Q_i(z^{(i)})$的计算公式就是后验概率，解决了$Q_i(z^{(i)})$如何选择得问题。这一步称为$E$步，建立 $\mathcal{L}(\theta)$得下界；接下来得$M$步，就是在给定$Q_i(z^{(i)})$后，调整$\theta$去极大化$\mathcal{L}(\theta)$的下界即
$$
\begin{aligned}
&\mathop{\arg\max}_{\theta}\sum\limits_{i=1}^m\log p(x^{(i)};\theta)\\
&\Leftrightarrow
\mathop{\arg\max}_{\theta}\sum\limits_{i=1}^m\sum\limits_{z^{(i)}}Q_i(z^{(i)})\log\dfrac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}\\
&\Leftrightarrow
\mathop{\arg\max}_{\theta}\sum\limits_{i=1}^m\sum\limits_{z^{(i)}}Q_i(z^{(i)})\left[\log p(x^{(i)},z^{(i)};\theta)-\log Q_i(z^{(i)})\right]\\
&\Leftrightarrow
\mathop{\arg\max}_{\theta}\sum\limits_{i=1}^m\sum\limits_{z^{(i)}}Q_i(z^{(i)})\log p(x^{(i)},z^{(i)};\theta)
\end{aligned}
$$
因此EM算法的迭代形式为\
Repeats until it converges{\
$E$ step：for every  $x^{(i)}$ calculate\
$Q_i(z^{(i)})=p(z^{(i)}|x^{(i)};\theta)$\
$M$ step：update  $\theta$
$\begin{aligned} \theta:=\mathop{\arg\max}_{\theta}\sum\limits_{i=1}^m\sum\limits_{z^{(i)}}Q_i(z^{(i)})\log p(x^{(i)},z^{(i)};\theta) \end{aligned}$\
}
<a id="markdown-收敛性证明" name="收敛性证明"></a>
### 收敛性证明
$$
\begin{aligned}
\mathcal{L}(\theta^{(k)})&=\sum\limits_{i=1}^m\sum\limits_{z^{(i)}}Q_i^{(k)}(z^{(i)})\log \dfrac{p(x^{(i)},z^{(i)};\theta^{(k)})}{Q_i(z^{(i)})} \ \ \ \ \ \ \ \ \ \ (a)\\
&\le \sum\limits_{i=1}^m\sum\limits_{z^{(i)}}Q_i^{(k)}(z^{(i)})\log \dfrac{p(x^{(i)},z^{(i)};\theta^{(k)})}{Q_i(z^{(i)})}\ \ \ \ \ \ \ \ \ \ (b)\\
&\le\mathcal{L}(\theta^{(k+1)})\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \  (c)
\end{aligned}
$$
首先$(a)$式是前面$E$步所保证詹森不等式中的等式成立的条件，$(a)$到$(b)$是M步的定义，$(b)$到$(c)$对任意参数都成立，而其等式的条件是固定$\theta$并调整好$Q$时成立，$(b)$到$(c)$只是固定$Q$调整$\theta$，在得到$\theta^{(k+1)}$时，只是最大化$\mathcal{L}(\theta^{(k)})$，也就是$\mathcal{L}(\theta^{(k+1)})$的一个下界而没有使等式成立。
<a id="markdown-结论" name="结论"></a>
### 结论
EM算法可以保证收敛到一个稳定点，但是却不能保证收敛到全局的极大值点，因此它是局部最优的算法，当然，如果我们的优化目标L(θ,θj)是凸的，则EM算法可以保证收敛到全局最大值，这点和梯度下降法这样的迭代算法相同。

<a id="markdown-gbdt-vs-xgbdt" name="gbdt-vs-xgbdt"></a>
## GBDT vs XGBDT
<a id="markdown-1目标函数" name="1目标函数"></a>
### 1、目标函数
GBDT目标函数最终形式为： 
$$ Obj^{(k)}=\sum\limits_{i=1}^m \left(l(y^{(i)},\hat{y}^{(i)}_{k-1})+g_if_k(x^{(i)})\right) $$

XGBDT目标函数最终形式为： 
$$ \begin{aligned} Obj^{(k)}&=\sum\limits_{i=1}^ml(y^{(i)},\hat{y}^{(i)}k)+\sum\limits_{i=1}^T{\Omega(f_i)}\ &=\sum\limits_{i=1}^ml(y^{(i)},\hat{y}^{(i)}_{k-1}+f_k(x^{(i)}))+\Omega(f_k)+C \end{aligned} $$

在决策树结构固定的情况下最优的目标函数:$Obj$:
$$ Obj=-\dfrac{1}{2}\sum\limits_{j=1}^T\dfrac{G_j^2}{H_j+\lambda}+\gamma T$$
然而决策树结构数量是无穷的，所以实际上并不能穷举所有可能的决策树结构，什么样的决策树结构是最优的呢？通常使用贪心策略来生成决策树的每个结点.

<a id="markdown-三l1l2正则" name="三l1l2正则"></a>
## 三、L1,L2正则
L1正则加上L1范数；L2正则加上L2范数
$L1$正则化和$L2$正则化的符号化描述假设待优化函数为$f(\theta)$，其中$\theta\in\R^n$，那么优化问题可以转化为求
$$\mathop{\arg\min}_{\theta}\ f(\theta)$$

$L1$正则化，即对参数$\theta$加上$L1$范数约束
$$\mathop{\arg\min}_{\theta}\ J_1(\theta)=f(\theta)+\color{magenta}\lambda\Vert\theta\Vert_1$$

$L2$正则化，即对参数$\theta$加上$L2$范数的平方约束
$$\mathop{\arg\min}_{\theta}\ J_2(\theta)=f(\theta)+\color{magenta}\lambda\Vert\theta\Vert_2^2 $$

<a id="markdown-四lstm与gru" name="四lstm与gru"></a>
## 四、LSTM与GRU
LSTM：忘记门+输入门+输出门
GRU：重置门+更新门

<a id="markdown-五pca降维" name="五pca降维"></a>
## 五、PCA降维
最大方差原理：
1. 对m\*n数据按列组成n*m；
2. 对数据，按行进行0均值化；
3. 求协方差矩阵
$$C=\dfrac{1}{m}XX^T\ $$
4. 对C求特征值，特征向量；按大小排列，取前K行
5. Y=P*X 为X降维后的K维数据

<a id="markdown-六感知机模型" name="六感知机模型"></a>
## 六、感知机模型

$$\begin{aligned} sign(x) = 
\begin{cases}
+1, & ifx \geqslant 0  \\
-1, &{otherwise}
\end{cases}
\end{aligned}$$
因此可知感知机的假设函数为$\psi (x)$,通过输入特征向量$x$即可判断其所属类别。

误分类点集$M$中所有点到超平面$S$的距离之和为
$$ -\dfrac{1}{\Vert \omega\Vert_2}\sum\limits_{(x^{(i)},y^{(i)})\in M}y^{(i)}(w^Tx^{(i)}+b)$$
所以求解最小化距离之和可知
$$\begin{aligned} \mathop{\arg\min}_{\omega, b} L(\omega,b) & \Leftrightarrow \mathop{\arg\min}_{\omega, b} -\dfrac{1}{\Vert \omega\Vert_2}\sum\limits_{(x^{(i)},y^{(i)})\in M}y^{(i)}(w^Tx^{(i)}+b)\ & \Leftrightarrow \mathop{\arg\min}_{\omega, b} -\sum\limits_{(x^{(i)},y^{(i)})\in M}y^{(i)}(w^Tx^{(i)}+b)\ \end{aligned}$$ 
综上可知，感知机算法的损失函数为
$$L(\omega,b)=-\sum\limits_{(x^{(i)},y^{(i)})\in M}y^{(i)}(w^Tx^{(i)}+b)$$






<a id="markdown-bp反向传播算法" name="bp反向传播算法"></a>
## BP反向传播算法
<a id="markdown-损失函数" name="损失函数"></a>
#### 损失函数
$$E_d\equiv\frac{1}{2}\sum_{i\in outputs}(t_i-y_i)^2$$
<a id="markdown-隐层输出" name="隐层输出"></a>
#### 隐层输出
$$net_{j}=\vec{w_j}·\vec{x_j}=\sum_{i}{w_{ji}}x_{ji}$$
<a id="markdown-梯度下降" name="梯度下降"></a>
#### 梯度下降
$$w_{ji}\gets w_{ji}-\eta\frac{\partial{E_d}}{\partial{w_{ji}}}$$
<a id="markdown-激活函数" name="激活函数"></a>
#### 激活函数
$$a_j=\sigma(net_j)$$
<a id="markdown-反向传播" name="反向传播"></a>
#### 反向传播
$$\frac{\partial{E_d}}{\partial{w_{ji}}}=\frac{\partial{E_d}}{\partial{net_j}}·\frac{\partial{net_j}}{\partial{w_{ji}}}\\
=\frac{\partial{E_d}}{\partial{net_j}}·\frac{\partial{\sum_{i}{w_{ji}}x_{ji}}}{\partial{w_{ji}}}\\
=\frac{\partial{E_d}}{\partial{net_j}}·x_{ji}$$
对于$\frac{\partial{E_d}}{\partial{net_j}}$的推导，需要区分输出层和隐藏层两种情况:

<a id="markdown-输出层i-j" name="输出层i-j"></a>
### 输出层$i->j$
$$\frac{\partial{E_d}}{\partial{net_j}}=\frac{\partial{E_d}}{\partial{y_j}}·\frac{\partial{y_j}}{\partial{net_j}}\\$$
$$\frac{\partial{E_d}}{\partial{y_j}}=\frac{\partial}{\partial{y_j}}\frac{1}{2}\sum_{i\in outputs}(t_i-y_i)^2
=\frac{\partial}{\partial{y_j}}\frac{1}{2}(t_j-y_j)^2
=-(t_j-y_j)$$
$$\frac{\partial{y_j}}{\partial{net_j}}=\frac{\partial sigmoid(net_j)}{\partial{net_j}}
=y_j(1-y_j)$$
故
$$\frac{\partial{E_d}}{\partial{net_j}}=-(t_j-y_j)y_j(1-y_j)$$
同时令
$$\delta_j=(t_j-y_j)y_j(1-y_j)$$


<a id="markdown-隐藏层j-k" name="隐藏层j-k"></a>
### 隐藏层$j->k$

$$\frac{\partial{E_d}}{\partial{net_j}}=\sum_{k\in Downstream(j)}\frac{\partial{E_d}}{\partial{net_k}}·\frac{\partial{net_k}}{\partial{net_j}}\\
=\sum_{k\in Downstream(j)}-\delta_k·\frac{\partial{net_k}}{\partial{net_j}}\\
=\sum_{k\in Downstream(j)}-\delta_k·\frac{\partial{net_k}}{\partial{a_j}}·\frac{\partial{a_j}}{\partial{net_j}}\\
=\sum_{k\in Downstream(j)}-\delta_k·w_{kj}·\frac{\partial{a_j}}{\partial{net_j}}\\
=\sum_{k\in Downstream(j)}-\delta_k·w_{kj}·a_j(1-a_j)\\
=-a_j(1-a_j)\sum_{k\in Downstream(j)}\delta_kw_{kj}$$

因为$\delta_j=-\frac{\partial{E_d}}{\partial{net_j}}$，带入上式得到：
$$\delta_j=a_j(1-a_j)\sum_{k\in Downstream(j)}\delta_kw_{kj}$$











