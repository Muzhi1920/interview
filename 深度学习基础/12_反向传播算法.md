## 反向传播算法
![avatar](http://upload-images.jianshu.io/upload_images/2256672-6f27ced45cf5c0d8.png)

损失函数：$E_d\equiv\frac{1}{2}\sum_{i\in outputs}(t_i-y_i)^2$

隐层输出：$net_{j}=\vec{w_j}·\vec{x_j}=\sum_{i}{w_{ji}}x_{ji}$

梯度下降：$w_{ji}\gets w_{ji}-\eta\frac{\partial{E_d}}{\partial{w_{ji}}}$

反向传播：
$$\frac{\partial{E_d}}{\partial{w_{ji}}}=\frac{\partial{E_d}}{\partial{net_j}}·\frac{\partial{net_j}}{\partial{w_{ji}}}\\
=\frac{\partial{E_d}}{\partial{net_j}}·\frac{\partial{\sum_{i}{w_{ji}}x_{ji}}}{\partial{w_{ji}}}\\
=\frac{\partial{E_d}}{\partial{net_j}}·x_{ji}$$

对于$\frac{\partial{E_d}}{\partial{net_j}}$的推导，需要区分输出层和隐藏层两种情况:
1. 输出层
$$\frac{\partial{E_d}}{\partial{net_j}}=\frac{\partial{E_d}}{\partial{y_j}}·\frac{\partial{y_j}}{\partial{net_j}}\\$$
$\frac{\partial{E_d}}{\partial{y_j}}=\frac{\partial}{\partial{y_j}}\frac{1}{2}\sum_{i\in outputs}(t_i-y_i)^2
=\frac{\partial}{\partial{y_j}}\frac{1}{2}(t_j-y_j)^2
=-(t_j-y_j)$
$\frac{\partial{y_j}}{\partial{net_j}}=\frac{\partial sigmoid(net_j)}{\partial{net_j}}
=y_j(1-y_j)$
故
$$\frac{\partial{E_d}}{\partial{net_j}}=-(t_j-y_j)y_j(1-y_j)$$
同时令
$$\delta_j=(t_j-y_j)y_j(1-y_j)
$$
2. 隐藏层
$$\frac{\partial{E_d}}{\partial{net_j}}=\sum_{k\in Downstream(j)}\frac{\partial{E_d}}{\partial{net_k}}·\frac{\partial{net_k}}{\partial{net_j}}\\
=\sum_{k\in Downstream(j)}-\delta_k·\frac{\partial{net_k}}{\partial{net_j}}\\
=\sum_{k\in Downstream(j)}-\delta_k·\frac{\partial{net_k}}{\partial{a_j}}·\frac{\partial{a_j}}{\partial{net_j}}\\
=\sum_{k\in Downstream(j)}-\delta_k·w_{kj}·\frac{\partial{a_j}}{\partial{net_j}}\\
=\sum_{k\in Downstream(j)}-\delta_k·w_{kj}·a_j(1-a_j)\\
=-a_j(1-a_j)\sum_{k\in Downstream(j)}\delta_kw_{kj}$$


因为$\delta_j=-\frac{\partial{E_d}}{\partial{net_j}}$，带入上式得到：
$$\delta_j=a_j(1-a_j)\sum_{k\in Downstream(j)}\delta_kw_{kj}$$




### 反向传播算法流程总结
##### (step1)   训练数据输入
假设训练数据集为($x^{(1)}$,$y^{(1)}$)，($x^{(2)}$,$y^{(2)}$)，$\cdots$，($x^{(r)}$,$y^{(r)}$)，$\cdots$，($x^{(m)}$,$y^{(m)}$)，其中$x^{(i)}\in\R^{n}$，并为输入层选择合适的激活函数$\sigma(x)$。

##### (step2)   前向传播过程
​对于神经网络的各层前向的计算一遍结果，$l=2,3,\cdots,L$。
$$
\begin{aligned}
\begin{cases}
\boldsymbol{z^{(l)}=w^{(l)}a^{(l-1)}+b}\\
\boldsymbol{a^{(l)}}=\sigma(\boldsymbol{z^{(l)}})

\end{cases}
\end{aligned}
$$

##### (step3)   计算输出层误差

$$
\boldsymbol{\delta^{(L)}}=\nabla_{\boldsymbol{a^{(L)}}}\boldsymbol{C(\theta)}\odot\boldsymbol{\sigma'(z^{(L)})}
$$

##### (step4)   计算反向传播误差

​对于神经网络的各层从后向前计算一遍结果，$l=L-1,L-2,\cdots,2$。
$$
\boldsymbol{\delta^{(l)}}=\left((\boldsymbol{w^{(l+1)}})^T\boldsymbol{\delta^{(l+1)}}\right)\odot\sigma'(\boldsymbol{z^{(l)}})
$$

##### (step5)   计算并且更新权重$w$和偏置$b$

​	通过梯度下降算法更新权重$w$和偏置$b$的值，$\alpha$为学习率其中$\alpha\in(0,1]$。
$$
\begin{aligned}
\begin{cases}
w_{jk}^{(l)}:=w_{jk}^{(l)}-\alpha\dfrac{\partial C(\theta)}{\partial w_{jk}^{(l)}}\\
b_j^{(l)}:=b_j^{(l)}-\alpha\dfrac{\partial C(\theta)}{\partial b_j^{(l)}}
\end{cases}
\end{aligned}
$$
即
$$
\begin{aligned}
\begin{cases}
w_{jk}^{(l)}:=w_{jk}^{(l)}-\alpha a_k^{(l-1)}\delta_j^{(l)}\\
b_j^{(l)}:=b_j^{(l)}-\alpha\delta_j^{(l)}
\end{cases}
\end{aligned}
$$

